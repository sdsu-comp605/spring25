{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Measuring Performance\n",
    "\n",
    "Last time:\n",
    "\n",
    "- Intro to Vectorization\n",
    "\n",
    "Today:\n",
    "\n",
    "1. [Measuring Performance](#measuring-performance)\n",
    "2. [Introduction to Performance Modeling](#introduction-to-performance-modeling)  \n",
    "  2.1 [The vector triad benchmark](#the-vector-triad-benchmark)  \n",
    "  2.2 [The STREAM benchmark](#the-stream-benchmark)\n",
    "\n",
    "## 1. Measuring Performance\n",
    "\n",
    "Two of the standard approaches to assessing performance:\n",
    "\n",
    " - Floating Point Operations per Second (FLOP/S)\n",
    "    - FLOPs = Floating Pointer Operations (careful on the small \"s\" vs big \"S\") (see \n",
    "      [video: 1.1.1 GFLOPS](https://youtu.be/cRyVMrNNRCk?si=mttXpzKWQVQP9sSB)).\n",
    "    - How many floating point operations (add, subtract, multiply, and maybe\n",
    "      divide) a code does per second.\n",
    "    - How to estimate: for matrix-matrix multiply we have 2 FLOP per inner loop,\n",
    "      and a triply nested loop thus total $\\text{FLOPs} = 2nmk$. The FLOPS are\n",
    "      then $\\text{FLOPS} = 2nmk / \\text{time}$. Note that usually we use\n",
    "      GigaFLOPS or GFLOPS: $\\text{GFLOPS} = 2nmk / \\text{time} / 10^9$.\n",
    "    - Calculate for one core of your machine:\n",
    "      $$\n",
    "      \\text{FLOP/S} = \\frac{\\text{cycle}}{\\text{second}} \\frac{\\text{FLOPs}}{\\text{cycle}} \n",
    "      $$\n",
    "      - cycles-per-second can be looked up for your CPU on your machine\n",
    "      - FLOP/S-per-cycle can be tricky to find, is based on your processors\n",
    "        microarchitecture and is most easily found on the [Wikipedia FLOP page](https://en.wikipedia.org/wiki/Floating_point_operations_per_second#FLOPs_per_cycle_for_various_processors).\n",
    "        For instance, consider a laptop with an Intel Core i5-8210Y CPU at 1.6 GHz. This\n",
    "        is an Amber Lake processors with 16 FLOPs-per-cycle, and thus has a\n",
    "        calculated FLOP rate of\n",
    "        $$\n",
    "          \\text{FLOPS} = 1.6 \\frac{\\text{Gigacycles}}{\\text{second}}\n",
    "                         16 \\frac{\\text{FLOPs}}{\\text{cycle}}\n",
    "        $$\n",
    "        (Note that some processors support turbo boost so you\n",
    "        may see even higher performance unless turbo boost is disabled)\n",
    "  - Memory Bandwidth:\n",
    "    - Rate at which memory can be moved to semiconductor memory (typically from\n",
    "      main memory).\n",
    "    - Can be looked up on manufacture page\n",
    "    - Measure in Gigabits-per-second (GiB/s) or GigaBytes-per-second (GB/s)\n",
    "      - GigaByte (GB): $1 \\text{ GB} = 10^9 \\text{ bytes} = 8 \\times 10^9 \\text{ bits}$\n",
    "      - Gigabit (GiB): $1 \\text{ GiB} = 10^9 \\text{ bits}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to Performance Modeling\n",
    "\n",
    "Models give is a conceptual and roughly quantitative framework by which to answer the following types of questions.\n",
    "\n",
    "* Why is an implementation exhibiting its observed performance?\n",
    "* How will performance change if we:\n",
    "  * optimize this component?\n",
    "  * buy new hardware? (Which new hardware?)\n",
    "  * run a different configuration?\n",
    "* While conceptualizing a new algorithm, what performance can we expect and what will be bottlenecks?\n",
    "\n",
    "Models are a guide for performance, but not an absolute.\n",
    "\n",
    "### Terms\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| $n$    | Input parameter related to problem size |\n",
    "| $W$    | Amount of work to solve problem $n$ |\n",
    "| $T$    | Execution time |\n",
    "| $R$    | Rate at which work is done |\n",
    "\n",
    "### 2.1 The vector triad benchmark\n",
    "\n",
    "- Fathoming the chief performance characteristics of a processor or system is one of the purposes of _low-level benchmarking_.\n",
    "\n",
    "- A low-level benchmark is a program that tries to test some specific feature of the architecture like, e.g., peak performance or memory bandwidth.\n",
    "\n",
    "- One of the prominent examples is the _vector triad_, introduced by Schönauer (self-edition, 2000). It comprises a nested loop, the inner level executing a multiply add operation on the elements of three vectors and storing the result in a fourth. \n",
    "\n",
    "#### Example in Fortran:\n",
    "\n",
    "See the following implementation as a Fortran code snippet:\n",
    "\n",
    "```Fortran\n",
    "double precision, dimension(N) :: A,B,C,D\n",
    "double precision :: S,E,MFLOPS\n",
    "\n",
    "do i=1,N                                !initialize arrays\n",
    "    A(i) = 0.d0; B(i) = 1.d0\n",
    "    C(i) = 2.d0; D(i) = 3.d0\n",
    "enddo\n",
    "\n",
    "call get_walltime(S)                    ! get time stamp\n",
    "\n",
    "do j=1,R\n",
    "    do i=1,N\n",
    "        A(i) = B(i) + C(i) * D(i)       ! 3 loads, 1 store\n",
    "    enddo \n",
    "    if(A(2).lt.0) call dummy(A,B,C,D)   ! prevent loop interchange\n",
    "enddo\n",
    "\n",
    "call get_walltime(E)                    ! get time stamp again\n",
    "\n",
    "MFLOPS = R*N*2.d0/((E-S)*1.d6)          ! compute MFlop/sec rate\n",
    "```\n",
    "\n",
    "- The purpose of this benchmark is to measure the performance of data transfers between memory and arithmetic units of a processor.\n",
    "\n",
    "- On the inner level, three _load streams_ for arrays `B`, `C` and `D` and one _store stream_ for `A` are active. \n",
    "\n",
    "- Depending on `N`, this loop might execute in a very small time, which would be hard to measure. The outer loop thus repeats the triad `R` times so that execution time becomes large enough to be accurately measurable. In practice one would choose `R` according to `N` so that the overall execution time stays roughly constant for different `N`.\n",
    "\n",
    "Observations:\n",
    "\n",
    "- The aim of the masked-out call to the `dummy()` subroutine is to prevent the compiler from doing an obvious optimization: Without the call, the compiler might discover that the inner loop does not depend at all on the outer loop index `j` and drop the outer loop right away.\n",
    "- The possible call to `dummy()` fools the compiler into believing that the arrays may change between outer loop iterations. This effectively prevents the optimization described, and the additional cost is negligible because the condition is always false (which the compiler does not know in advance).\n",
    "- Please note that the most sensible time measure in benchmarking is _wallclock time_, also called _elapsed time_. Any other \"time\" that the system may provide, first and foremost the much stressed CPU time, is prone to misinterpretation because there\n",
    "might be contributions from I/O, context switches, other processes, etc., which CPU time cannot encompass (this is especially true for parallel programs).\n",
    "\n",
    "![vector triad performance graph](../img/vector_triad_perf_graph.png \"Vector triad performance graph\")\n",
    "\n",
    "In the above graph, we see the serial vector triad performance versus loop length for several generations of In-\n",
    "tel processor architectures (clock speed and year of introduction is indicated in the legend), and the NEC\n",
    "SX-8 vector processor. Note the entirely different performance characteristics of the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "- If you are not familiar, check this reference on [How to Compile a C program](https://sdsu-comp526.github.io/fall24/slides/module2-1_intro_to_c.html#compiling-a-c-program) and [How to Compile a Fortran program](https://sdsu-comp526.github.io/fall24/slides/module6-1_intro_to_fortran.html#how-to-compile-a-fortran-program)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The [STREAM benchmark](https://www.amd.com/en/developer/zen-software-studio/applications/spack/stream-benchmark.html):\n",
    "\n",
    "A C snippet:\n",
    "\n",
    "```c\n",
    "for (i=0; i<n; i++)\n",
    "    a[i] = b[i] + scalar*c[i];\n",
    "```\n",
    "\n",
    "Following the notation above, $n$ is the array size and\n",
    "\n",
    "$$W = 3 \\cdot \\texttt{sizeof(double)} \\cdot n$$\n",
    "\n",
    "is the number of bytes transferred. \n",
    "\n",
    "The rate $R = W/T$ is measured in bytes per second (or MB/s, etc.).\n",
    "\n",
    "#### Dense matrix multiplication\n",
    "\n",
    "To perform the operation $C \\gets C + A B$ where $A,B,C$ are $n\\times n$ matrices. Again, a C snippet would look like:\n",
    "\n",
    "```c\n",
    "for (i=0; i<n; i++)\n",
    "    for (j=0; j<n; j++)\n",
    "        for (k=0; k<n; k++)\n",
    "            c[i*n+j] += a[i*n+k] * b[k*n+j];\n",
    "```\n",
    "\n",
    "* Can you identify two expressions for the total amount of work $W(n)$ and the associated units?\n",
    "\n",
    "* Can you think of a context in which one is better than the other and vice-versa?\n",
    "\n",
    ":::{note}\n",
    "- C is said to follow the **row-major** order method. \n",
    "- Fortran, MATLAB, and Julia follow the **column-major** order method. \n",
    "\n",
    "If you are not familiar with row/column-major order methods, please see this [Wiki page](https://en.wikipedia.org/wiki/Row-_and_column-major_order).\n",
    "\n",
    "- Support for multi-dimensional arrays may also be provided by external libraries, which may even support arbitrary orderings, where each dimension has a stride value, and row-major or column-major are just two possible resulting interpretations. For example, row-major order is the default in NumPy (for Python) even though Python itself is neither row or column major (uses lists of lists); Similarly, column-major order is the default in Eigen and Armadillo (libraries for C++).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating time\n",
    "\n",
    "To estimate time, we need to know how fast hardware executes flops and moves bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "using Plots\n",
    "default(linewidth=4, legendfontsize=12)\n",
    "\n",
    "hardware = CSV.read(\"../assets/data/data-intel.csv\", DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(hardware[:,3], hardware[:,5], xlabel = \"GFLOPs Double Precision\", ylabel = \"Memory GBps\", primary=false)\n",
    "scatter!(hardware[:,3], hardware[:,5], label = \"Mem-GBps\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have rates $R_f = 4660 \\cdot 10^9$ flops/second and $R_m = 175 \\cdot 10^9$ bytes/second.  Now we need to characterize some algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algs = CSV.read(\"../assets/data/algs.csv\", DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the operations per byte, or **operational intensity**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algs[!, :intensity] .= algs[:,3]  ./ algs[:,2]\n",
    "algs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort!(algs, :intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function exec_time(machine, alg, n)\n",
    "    bytes = n * alg.bytes\n",
    "    flops = n * alg.flops\n",
    "    machine = DataFrame(machine)\n",
    "    T_mem = bytes ./ (machine[:, \"Mem-GBps\"] * 1e9)\n",
    "    T_flops = flops ./ (machine[:, \"GFLOPs-DP\"] * 1e9)\n",
    "    return maximum([T_mem[1], T_flops[1]])\n",
    "end\n",
    "\n",
    "Xeon_Platinum_9282 = filter(:Name => ==(\"Xeon Platinum 9282\"), hardware)\n",
    "SpMV = filter(:Name => ==(\"SpMV\"), algs)\n",
    "\n",
    "exec_time(Xeon_Platinum_9282, SpMV, 1e8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = plot()\n",
    "for machine in eachrow(hardware)\n",
    "    for alg in eachrow(algs)\n",
    "        ns = exp10.(range(4,9,length=6))\n",
    "        times = [exec_time(machine, alg, n) for n in ns]\n",
    "        flops = [alg.flops .* n for n in ns]\n",
    "        rates = flops ./ times\n",
    "        plot!(ns, rates, linestyle = :solid, marker = :circle, label=\"\", xscale=:log10, yscale=:log10)\n",
    "    end\n",
    "end\n",
    "xlabel!(\"n\")\n",
    "ylabel!(\"rate\")\n",
    "display(pl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like performance does not depend on problem size.\n",
    "\n",
    "Well, yeah, we chose a model in which flops and bytes were both proportional to $n$, and our machine model has no sense of cache hierarchy or latency, so time is also proportional to $n$.  We can divide through by $n$ and yield a more illuminating plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for machine in eachrow(hardware)\n",
    "    times = [exec_time(machine, alg, 1) for alg in eachrow(algs)]\n",
    "    rates = algs.flops ./ times\n",
    "    intensities = algs.intensity\n",
    "    plot!(intensities, rates, xscale=:log10, yscale=:log10, marker = :o, label = machine.Name, legend = :outertopright)\n",
    "end\n",
    "\n",
    "xlabel!(\"intensity\")\n",
    "ylabel!(\"rate\")\n",
    "ylims!(1e9, 5e12)\n",
    "xlims!(5e-2,1e1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We're seeing the **roofline** for the older processors while the newer models are memory bandwidth limited for all of these algorithms. \n",
    "- The _flat_ part of the roof means that performance is _compute-bound_. The _slanted_ part of the roof means performance is _memorybound_.\n",
    "- Note that the ridge point (where the diagonal and horizontal roofs meet) offers insight into the computer’s overall performance. \n",
    "  * The x-coordinate of the ridge point is the minimum operational intensity required to achieve maximum performance. \n",
    "  * If the ridge point is far to the right, then only kernels with very high operational intensity can achieve the maximum performance of that computer. \n",
    "  * If it is far to the left, then almost any kernel can potentially hit maximum performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommended reading on single-node performance modeling:\n",
    "\n",
    "> * [Williams, Waterman, Patterson (2009): **Roofline: An insightful visual performance model for multicore architectures**](https://doi.org/10.1145/1498765.1498785)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.6",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
