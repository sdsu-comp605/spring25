{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83d3d99e",
   "metadata": {},
   "source": [
    "# 28) Intro to GPU programming in Julia \n",
    "\n",
    "Last time:\n",
    "- Practical CUDA\n",
    "- Memory\n",
    "- Tuckoo demo for CUDA codes\n",
    "\n",
    "Today:\n",
    "1. Intro to GPU programming in Julia (CUDA.jl)  \n",
    "2. CUDA Julia code on the tuckoo cluster  \n",
    "   2.1 Launching a CUDA.jl code with SLURM  \n",
    "3. Parallel CUDA Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3fdc47",
   "metadata": {},
   "source": [
    "## 1. Intro to GPU programming in Julia (CUDA.jl)\n",
    "\n",
    "For GPU programming there are many great resources. Some that I may refer to are:\n",
    "\n",
    ":::{note} References:\n",
    "- [Warburton | youtube video](https://www.youtube.com/watch?v=uvVy3CqpVbM) (In the first 47 minutes of the video, Tim gives an excellent introduction to the GPU.)\n",
    "- [Warburton | ATPESC pdf](https://extremecomputingtraining.anl.gov/files/2018/08/ATPESC_2018_Track-2_3_8-2_830am_Warburton-Accelerators.pdf)\n",
    ":::\n",
    "\n",
    "### Add vectors\n",
    "\n",
    "The real \"hello world\" of the GPU is adding vectors:\n",
    "\n",
    "$$\n",
    "C = A + B\n",
    "$$\n",
    "\n",
    "Big ideas:\n",
    "\n",
    "- threads\n",
    "- blocks\n",
    "- how to launch and time kernels\n",
    "- off-loaded memory on the device\n",
    "\n",
    "### Example with [CUDA.jl](https://github.com/JuliaGPU/CUDA.jl)\n",
    "\n",
    "To run this and following Julia CUDA examples, you need to first add [CUDA.jl](https://github.com/JuliaGPU/CUDA.jl) to your environment. That is, start `julia` and in the REPL run:\n",
    "\n",
    "```julia\n",
    "using Pkg\n",
    "Pkg.add(\"CUDA\")\n",
    "```\n",
    "\n",
    "Then you can execute the following script:\n",
    "\n",
    "\n",
    "```{literalinclude} ../julia_codes/module8-1/add_cu_arrays.jl\n",
    ":language: julia\n",
    ":linenos: true\n",
    "```\n",
    "\n",
    "### Memory management\n",
    "\n",
    "As we have seen so far, a crucial aspect of working with a GPU is managing the data on it. \n",
    "\n",
    "The `CuArray` type is the primary interface for doing so: Creating a `CuArray` will allocate data on the GPU, copying elements to it will upload, and converting back to an `Array` will download values to the CPU. Let's see it in an example test. \n",
    "\n",
    "Note: to run the following test, you first need to also add the `Test.jl` package to your environment:\n",
    "\n",
    "\n",
    "```julia\n",
    "using Pkg\n",
    "Pkg.add(\"Test\")\n",
    "```\n",
    "\n",
    "Then you can run the following test example:\n",
    "\n",
    "```{literalinclude} ../julia_codes/module8-1/copy_cu_array.jl\n",
    ":language: julia\n",
    ":linenos: true\n",
    "```\n",
    "\n",
    "**Observation on garbage collection**:\n",
    "\n",
    "- One striking difference between the native C CUDA implementation and the Julia CUDA.jl interface is that instances of the `CuArray` type are managed by the Julia garbage collector. This means that they will be collected once they are unreachable, and the memory hold by it will be repurposed or freed. There is _no need_ for manual memory management (like the `cudaFree`), just make sure your objects are not reachable (i.e., there are no instances or references).\n",
    "\n",
    "### Reverse vectors?\n",
    "\n",
    "The \"hello world 2.0\" of the GPU is (inplace) reverse vector\n",
    "\n",
    "$$\n",
    "A_i := A_{N - i + 1}, \\textrm{with } i = 1, \\ldots, N\n",
    "$$\n",
    "\n",
    "Big ideas:\n",
    "\n",
    "- thread independence\n",
    "- [race conditions](https://en.wikipedia.org/wiki/Race_condition)\n",
    "\n",
    "\n",
    "### CUDA.jl API [Overview](https://cuda.juliagpu.org/stable/usage/overview/)\n",
    "\n",
    "The CUDA.jl package provides three distinct, but related, interfaces for CUDA programming:\n",
    "\n",
    "- the `CuArray` type: for programming with arrays;\n",
    "- native kernel programming capabilities: for writing CUDA kernels in Julia;\n",
    "- CUDA API wrappers: for low-level interactions with the CUDA libraries.\n",
    "\n",
    "Much of the Julia CUDA programming stack can be used by just relying on the `CuArray` type, and using platform-agnostic programming patterns like `broadcast` and other array abstractions. Only once you hit a performance bottleneck, or some missing functionality, you might need to write a custom kernel or use the underlying CUDA APIs.\n",
    "\n",
    "### CUDA.jl [kernel programming](https://cuda.juliagpu.org/stable/development/kernel/#Kernel-programming)\n",
    "\n",
    "You can write your own GPU kernels in Julia.\n",
    "\n",
    "- CUDA.jl aims to expose the full power of the CUDA programming model, i.e., at the same level of abstraction as CUDA C/C++, albeit with some Julia-specific improvements.\n",
    "\n",
    "#### Defining and launching kernels\n",
    "\n",
    "Kernels are written as ordinary Julia functions, but they **have to return** `nothing`:\n",
    "\n",
    "```julia\n",
    "    function my_kernel()\n",
    "        return\n",
    "    end\n",
    "```\n",
    "\n",
    "To launch this kernel, use the @cuda macro:\n",
    "\n",
    "```julia\n",
    "    julia> @cuda my_kernel()\n",
    "```\n",
    "\n",
    "##### Kernel inputs and outputs\n",
    "\n",
    "- GPU kernels cannot return values, and should always `return` or `return nothing`.\n",
    "\n",
    "- To communicate values from a kernel, you can use a `CuArray`:\n",
    "\n",
    "Example:\n",
    "\n",
    "```julia\n",
    "function my_kernel(a)\n",
    "    a[1] = 42\n",
    "    return\n",
    "end\n",
    "```\n",
    "\n",
    "Then you can launch it via:\n",
    "\n",
    "```julia\n",
    "julia> a = CuArray{Int}(undef, 1);\n",
    "```\n",
    "\n",
    "```julia\n",
    "julia> @cuda my_kernel(a);\n",
    "```\n",
    "\n",
    "```julia\n",
    "julia> a\n",
    "1-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n",
    " 42\n",
    "```\n",
    "\n",
    "#### Launch configuration and indexing\n",
    "\n",
    "- Simply using `@cuda` only launches a _single thread_, which is not very useful. To launch more threads, use the `threads` and `blocks` keyword arguments to `@cuda`. Example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ba789",
   "metadata": {},
   "source": [
    "## 2. CUDA Julia code on the tuckoo cluster\n",
    "\n",
    "As we have seen in the last [lecture](https://sdsu-comp605.github.io/spring25/lectures/module7-3_practical_cuda.html#tuckoo-demo-for-cuda-codes), we can run CUDA code on the tuckoo cluster. \n",
    "\n",
    "Now let's learn how to [configure](https://cuda.juliagpu.org/stable/installation/overview/#Specifying-the-CUDA-version) the `CUDA.jl` package to run with the proper CUDA version available on the cluster.\n",
    "\n",
    "Recall that for tuckoo, **`node8`** is the node dedicated for interactive usage, where you can find the `nvcc` compiler to compile CUDA codes.\n",
    "\n",
    "Once you are logged in on tuckoo using \n",
    "\n",
    "```shell\n",
    "ssh your_user_name@tuckoo.sdsu.edu\n",
    "```\n",
    "\n",
    "You will see that you are logged in on the _login node_ because the prompt will show\n",
    "\n",
    "```shell\n",
    "[your_user_name@tuckoo ~]$\n",
    "```\n",
    "\n",
    "Now you can use `rsh` to connect to `node8` by simply running:\n",
    "\n",
    "```shell\n",
    "rsh node8\n",
    "```\n",
    "\n",
    "and you will see that you are indeed on `node8` because the prompt has changed to:\n",
    "\n",
    "```shell\n",
    "[your_user_name@node8 ~]$\n",
    "```\n",
    "\n",
    "- Now you can start a Julia session with a local environment in a  directory of your choice, by running \n",
    "\n",
    "```julia\n",
    "julia --project=.\n",
    "```\n",
    "\n",
    "Recall that this will generate a `Project.toml` and `Manifest.toml` file in the directory in which you have specified the `--project=` path. \n",
    "\n",
    "- Once your Julia session has started, you can add the `CUDA.jl` and `Test.jl` packages needed to test your code. \n",
    "\n",
    "```julia\n",
    "Pkg.add(\"CUDA\")\n",
    "Pkg.add(\"Test\")\n",
    "```\n",
    "\n",
    "Now if you try to use the `CUDA.jl` package by running `using CUDA`, the first time you'll hit the following error:\n",
    "\n",
    "```julia\n",
    "julia> using CUDA\n",
    "┌ Error: CUDA.jl was precompiled without knowing the CUDA toolkit version. This is unsupported.\n",
    "│ You should either precompile CUDA.jl in an environment where the CUDA toolkit is available,\n",
    "│ or call `CUDA.set_runtime_version!` to specify which CUDA version to use.\n",
    "└ @ CUDA ~/.julia/packages/CUDA/TW8fL/src/initialization.jl:148\n",
    "```\n",
    "\n",
    "- Run the `CUDA.versioninfo()` command to see which CUDA version is available on the cluster:\n",
    "```julia\n",
    "julia> CUDA.versioninfo()\n",
    "```\n",
    "\n",
    "You will see the following output:\n",
    "\n",
    "```julia\n",
    "julia> CUDA.versioninfo()\n",
    "CUDA runtime 12.0, local installation\n",
    "CUDA driver 12.0\n",
    "NVIDIA driver 525.60.13\n",
    "\n",
    "CUDA libraries: \n",
    "- CUBLAS: 12.0.1\n",
    "- CURAND: 10.3.1\n",
    "- CUFFT: 11.0.0\n",
    "- CUSOLVER: 11.4.2\n",
    "- CUSPARSE: 12.0.0\n",
    "- CUPTI: 2022.4.0 (API 18.0.0)\n",
    "- NVML: 12.0.0+525.60.13\n",
    "\n",
    "Julia packages: \n",
    "- CUDA: 5.7.2\n",
    "- CUDA_Driver_jll: 0.12.1+1\n",
    "- CUDA_Runtime_jll: 0.16.1+0\n",
    "- CUDA_Runtime_Discovery: 0.3.5\n",
    "\n",
    "Toolchain:\n",
    "- Julia: 1.11.4\n",
    "- LLVM: 16.0.6\n",
    "\n",
    "Preferences:\n",
    "- CUDA_Runtime_jll.local: true\n",
    "\n",
    "2 devices:\n",
    "  0: Tesla P100-PCIE-16GB (sm_60, 15.892 GiB / 16.000 GiB available)\n",
    "  1: Tesla P100-PCIE-16GB (sm_60, 15.892 GiB / 16.000 GiB available)\n",
    "```\n",
    "\n",
    "- Therefore, we want to set the **`v12.0`** version for our `CUDA.jl` library. Let's do it by running the following command:\n",
    "\n",
    "```julia\n",
    "julia> CUDA.set_runtime_version!(v\"12.0\")\n",
    "```\n",
    "\n",
    "It will print the following output:\n",
    "\n",
    "```julia\n",
    "julia> CUDA.set_runtime_version!(v\"12.0\")\n",
    "[ Info: Configure the active project to use CUDA 12.0; please re-start Julia for this to take effect.\n",
    "```\n",
    "\n",
    "- Now your Julia environment on the cluster is properly setup to use the right CUDA version. \n",
    "\n",
    "- You will notice that your local `Project.toml` and `Manifest.toml` files have changed and now include the `CUDA.jl` dependency. \n",
    "\n",
    "### 2.1 Launching a CUDA.jl code with SLURM\n",
    "\n",
    "Once you have verified that your local `Project.toml` and `Manifest.toml` files have been populated correctly, you can copy the [julia_codes/module8-1/copy_cu_array.jl](https://github.com/sdsu-comp605/spring25/blob/main/julia_codes/module8-1/copy_cu_array.jl) and [batch_scripts/batch.cuda-julia](https://github.com/sdsu-comp605/spring25/tree/main/batch_scripts/batch.cuda-julia) files in the same directory where you have the `Project.toml` and `Manifest.toml` files.\n",
    "\n",
    "This is the SLURM batch script to setup and precompile your Julia environment and launch Julia CODE on a P100 on tuckoo:\n",
    "\n",
    "```{literalinclude} ../batch_scripts/batch.cuda-julia\n",
    ":language: shell\n",
    ":linenos: true\n",
    "```\n",
    "\n",
    "\n",
    "You can simply launch your SLURM batch script with\n",
    "\n",
    "```shell\n",
    "sbatch batch.cuda-julia\n",
    "```\n",
    "\n",
    "and it should produce the following output:\n",
    "\n",
    "```shell\n",
    "Status `~/comp605/spring25/julia_codes/module8-1/Project.toml`\n",
    "  [052768ef] CUDA v5.7.3\n",
    "  [8dfed614] Test v1.11.0\n",
    "Begin test. \n",
    "Test ended. \n",
    "Test Summary: | Pass  Total  Time\n",
    "CopyCuArray   |    1      1  1.0s\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a47a5b8",
   "metadata": {},
   "source": [
    "## 3. Parallel Julia CUDA\n",
    "\n",
    ":::{tip}\n",
    "- **Reference**: [CUDA.jl tutorials](https://cuda.juliagpu.org/v2.5/tutorials/introduction/)\n",
    ":::\n",
    "\n",
    "### [Tasks and threads](https://cuda.juliagpu.org/stable/usage/multitasking/)\n",
    "\n",
    "CUDA.jl can be used with Julia tasks and threads, offering a convenient way to work with multiple devices, or to perform independent computations that may execute concurrently on the GPU.\n",
    "\n",
    "#### Task-based programming\n",
    "\n",
    "\n",
    "Each Julia task gets its own local CUDA execution environment, with its own stream, library handles, and active device selection. That makes it easy to use one task per device, or to use tasks for independent operations that can be overlapped. At the same time, it's important to take care when sharing data between tasks.\n",
    "\n",
    "For example, let's take some dummy expensive computation and execute it from two tasks:\n",
    "\n",
    "```{literalinclude} ../julia_codes/module8-1/sync_tasks.jl\n",
    ":language: julia\n",
    ":linenos: true\n",
    "```\n",
    "\n",
    "- In the above example, we create two tasks and re-synchronize afterwards (`@async` and `@sync`), while the dummy compute function demonstrates both the use of a library (matrix multiplication uses CUBLAS) and a native Julia kernel. \n",
    "\n",
    "- The `main` function illustrates how we need to take care when sharing data between tasks: \n",
    "  * GPU operations typically execute **asynchronously**, queued on an execution stream, so if we switch tasks and thus switch execution streams we need to `synchronize()` to ensure the data is actually available."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
