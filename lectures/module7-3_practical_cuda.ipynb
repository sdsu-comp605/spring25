{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f134ec",
   "metadata": {},
   "source": [
    "# 27) Practical CUDA\n",
    "\n",
    "Last Time:\n",
    "\n",
    "- GPUs and CUDA\n",
    "- Kernel syntax examples\n",
    "- Thread hirerachy\n",
    "- Memory\n",
    "\n",
    "Today:\n",
    "\n",
    "1. When to use a GPU?  \n",
    "2. Practical CUDA  \n",
    "3. Memory   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfcf382",
   "metadata": {},
   "source": [
    "## 1. When to use a GPU?\n",
    "\n",
    "* GPUs have 2-4x greater floating point and bandwidth peak for the watts\n",
    "  * also for the \\$ if you buy enterprise gear\n",
    "  * better for the \\$ if you buy gaming gear\n",
    "* Step 1 is to assess workload and latency requirements\n",
    "\n",
    "![](VecDot_CPU_vs_GPU_size.png)\n",
    "![](VecDot_CPU_vs_GPU_time.png)\n",
    "\n",
    "* Don't waste time with GPUs if\n",
    "  * your problem size or time to solution requirements don't align\n",
    "  * if the work you'd like to move to the GPU is not a bottleneck\n",
    "  * if the computation cost will be dwarfed by moving data to/from the GPU\n",
    "    * often you need to restructure so that caller passes in data already on the device\n",
    "    * can require nonlocal refactoring\n",
    "* Almost never: pick one kernel at a time and move it to the GPU\n",
    "  * Real-world examples: DOE ACME/E3SM projects (to pick on one high-profile application) has basically done this for five years and it still doesn't help their production workloads so they bought a non-GPU machine\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1419a410",
   "metadata": {},
   "source": [
    "### Okay, okay, okay.  What if I have the right workload?\n",
    "\n",
    "#### Terminology/Intro\n",
    "\n",
    "* [An even easier introduction to CUDA](https://devblogs.nvidia.com/even-easier-introduction-cuda/)\n",
    "* [CUDA Programming Model](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model)\n",
    "\n",
    "* On the CPU, we have a thread with vector registers/instructions\n",
    "* In CUDA, we write code inside a single vector lane (\"confusingly\" called a CUDA thread)\n",
    "* To get inside the lane, we launch a **kernel** from the CPU using special syntax. For example:\n",
    "\n",
    "```c\n",
    "add<<<numBlocks, blockSize>>>(N, x, y);\n",
    "```\n",
    "\n",
    "* needs to be compiled using `nvcc` compiler\n",
    "* Logically 1D/2D/3D rectangular tiled iteration space\n",
    "\n",
    "![CUDA: grid of thread blocks](../img/grid-of-thread-blocks.png \"CUDA: grid of thread blocks\")\n",
    "\n",
    "\n",
    "* There are [many](https://en.wikipedia.org/wiki/CUDA#Version_features_and_specifications) constraints and limitations to the iteration \"grid\"\n",
    "\n",
    "![CUDA constraints](../img/cuda-constraints.png \"CUDA constraints\")\n",
    "\n",
    "* Control flow for CUDA threads is nominally independent, but performance will be poor if you don't coordinate threads within each block.\n",
    "  * Implicit coordination:\n",
    "    * Memory coalescing\n",
    "    * Organize your algorithm to limit \"divergence\"\n",
    "  * Explicit coordination:\n",
    "    * Shared memory\n",
    "    * `__syncthreads()`\n",
    "    * Warp shuffles\n",
    "* We implement the kernel by using the `__global__` attribute\n",
    "  * Visible from the CPU\n",
    "  * Special [built-in variables](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#built-in-variables) are defined\n",
    "    * `gridDim`: dimension of the grid\n",
    "    * `blockIdx`: block index within the grid\n",
    "    * `blockDim`: dimensions of the block\n",
    "    * `threadIdx`: thread index within the block.\n",
    "  * There is also `__device__`, which is callable from other device functions\n",
    "  * Can use `__host__ __device__` to compile two versions\n",
    "\n",
    "![CUDA indexing](../img/cuda_indexing.png \"CUDA indexing\")\n",
    "\n",
    "#### How does this relate to the hardware?\n",
    "\n",
    "* Each thread block is assigned to one **streaming multiprocessor (SM)**\n",
    "* Executed in warps (number of hardware lanes)\n",
    "* Multiple warps (from the same or different thread blocks) execute like \"hyperthreads\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e7efd",
   "metadata": {},
   "source": [
    "## 2. Practical CUDA  \n",
    "\n",
    "### [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html)\n",
    "\n",
    "#### Occupancy\n",
    "\n",
    "> Thread instructions are executed sequentially in CUDA, and, as a result, executing other warps when one warp is paused or stalled is the only way to hide latencies and keep the hardware busy. Some metric related to the number of active warps on a multiprocessor is therefore important in **determining how effectively the hardware is kept busy**. This metric is _occupancy_.  [emphasis added]\n",
    "\n",
    "* Reality: occupancy is just one aspect, and often inversely correlated with keeping the hardware busy (and with performance).\n",
    "\n",
    "> Occupancy is the ratio of the number of active warps per multiprocessor to the maximum number of possible active warps.\n",
    "\n",
    "* If your kernel uses fewer registers/less shared memory, more warps can be scheduled.\n",
    "* Register/shared memory usage is determined by the compiler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f2da48",
   "metadata": {},
   "source": [
    "Code example: \n",
    "\n",
    "```{literalinclude} ../cuda_codes/module7-3/add.cu\n",
    ":language: cuda\n",
    ":linenos: true\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8fdb3f",
   "metadata": {},
   "source": [
    "! nvcc ../cuda_codes/lecture7-3/add.cu --resource-usage -o add"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
