
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>25) Practical CUDA &#8212; Scientific Computing</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/module7-3_practical_cuda';</script>
    <link rel="icon" href="../_static/SDSU-comp605_logo.svg"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="26) Intro to GPU programming in Julia" href="module8-1_cuda_jl.html" />
    <link rel="prev" title="24) GPUs and CUDA" href="module7-2_cuda.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/SDSU-comp605_logo.svg" class="logo__image only-light" alt="Scientific Computing - Home"/>
    <script>document.write(`<img src="../_static/SDSU-comp605_logo.svg" class="logo__image only-dark" alt="Scientific Computing - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Welcome
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../syllabus.html">Syllabus</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../lectures.html">Lectures</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="module1-1_first_class.html">1) First Class: Reproducibility and Git</a></li>


<li class="toctree-l2"><a class="reference internal" href="module1-2_linux.html">2) The Linux Filesystem and commands</a></li>
<li class="toctree-l2"><a class="reference internal" href="module1-3_community_projects.html">3) Community Projects</a></li>
<li class="toctree-l2"><a class="reference internal" href="module1-4_intro_architectures.html">4) Introduction to Computer Architectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="module1-5_intro_vectorization.html">5) Introduction to Vectorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="module2-1_measuring_performance.html">6) Measuring Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="module2-2_cpu_optimization.html">7) CPU Optimization: Matrix-Matrix Multiply</a></li>
<li class="toctree-l2"><a class="reference internal" href="module2-3_blocked_mmm.html">8) Blocked Matrix-Matrix Multiplication</a></li>
<li class="toctree-l2"><a class="reference internal" href="module2-4_packing_for_cache.html">9) Packing for cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="module2-5_intro_multithreading.html">10) Introduction to Multithreading</a></li>
<li class="toctree-l2"><a class="reference internal" href="module3-1_intro_parallel_scaling.html">11) Introduction to Parallel Scaling</a></li>
<li class="toctree-l2"><a class="reference internal" href="module3-2_intro_to_openmp.html">12) Introduction to OpenMP</a></li>
<li class="toctree-l2"><a class="reference internal" href="module3-3_openmp_tasks.html">13) More on OpenMP and OpenMP Tasks</a></li>
<li class="toctree-l2"><a class="reference internal" href="module4-1_case_study_1_libceed.html">14) Case Study 1: libCEED</a></li>
<li class="toctree-l2"><a class="reference internal" href="module4-2_case_study_2_climacore.html">15) Case Study 2: ClimaCore</a></li>
<li class="toctree-l2"><a class="reference internal" href="module5-1_reductions_and_scans.html">16) Parallel reductions and scans</a></li>
<li class="toctree-l2"><a class="reference internal" href="module5-2_intro_to_mpi.html">17) Introduction to MPI</a></li>
<li class="toctree-l2"><a class="reference internal" href="module5-3_intro_to_slurm.html">18) Introduction to Batch Jobs</a></li>
<li class="toctree-l2"><a class="reference internal" href="module5-4_parallel_linear_alegebra.html">19) Parallel Linear Algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="module6-1_intro_to_mpi_jl.html">20) Introduction to MPI.jl</a></li>
<li class="toctree-l2"><a class="reference internal" href="module6-2_blocking_vs_nonblocking.html">21) Blocking and non-blocking communication</a></li>
<li class="toctree-l2"><a class="reference internal" href="module6-3_collectives.html">22) Collective communication</a></li>
<li class="toctree-l2"><a class="reference internal" href="module7-1_coprocessors.html">23) Coprocessor architectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="module7-2_cuda.html">24) GPUs and CUDA</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">25) Practical CUDA</a></li>
<li class="toctree-l2"><a class="reference internal" href="module8-1_cuda_jl.html">26) Intro to GPU programming in Julia</a></li>
<li class="toctree-l2"><a class="reference internal" href="module8-2_cuda_jl_memory.html">27) Memory management with CUDA.jl</a></li>
<li class="toctree-l2"><a class="reference internal" href="module8-3_cuda_jl_reduction.html">28) Parallel reductions with CUDA.jl</a></li>
<li class="toctree-l2"><a class="reference internal" href="module9-1_ispc_and_others.html">29) ISPC, OpenMP target, OpenACC, and all that</a></li>
<li class="toctree-l2"><a class="reference internal" href="module9-2_parallel_io.html">30) I/O in HPC</a></li>
<li class="toctree-l2"><a class="reference internal" href="module10-1_careers.html">31) Careers in HPSC</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/sdsu-comp605/spring25" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/sdsu-comp605/spring25/issues/new?title=Issue%20on%20page%20%2Flectures/module7-3_practical_cuda.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/module7-3_practical_cuda.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>25) Practical CUDA</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-use-a-gpu">1. When to use a GPU?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#okay-okay-okay-what-if-i-have-the-right-workload">Okay, okay, okay.  What if I have the right workload?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#terminology-intro">Terminology/Intro</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-this-relate-to-the-hardware">How does this relate to the hardware?</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">2. Practical CUDA</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cuda-best-practices-guide">CUDA Best Practices Guide</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#occupancy">Occupancy</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further reading:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#memory">3. Memory</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unified-managed-memory">Unified/managed memory</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#on-memory-coalescing-and-strided-access">3.1 On memory coalescing and strided access</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tuckoo-demo-for-cuda-codes">4. Tuckoo demo for CUDA codes</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="practical-cuda">
<h1>25) Practical CUDA<a class="headerlink" href="#practical-cuda" title="Link to this heading">#</a></h1>
<p>Last Time:</p>
<ul class="simple">
<li><p>GPUs and CUDA</p></li>
<li><p>Kernel syntax examples</p></li>
<li><p>Thread hirerachy</p></li>
<li><p>Memory</p></li>
</ul>
<p>Today:</p>
<ol class="arabic simple">
<li><p>When to use a GPU?</p></li>
<li><p>Practical CUDA</p></li>
<li><p>Memory<br />
3.1 On memory coalescing and strided access</p></li>
<li><p>Tuckoo demo for CUDA codes</p></li>
</ol>
<section id="when-to-use-a-gpu">
<h2>1. When to use a GPU?<a class="headerlink" href="#when-to-use-a-gpu" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>GPUs have 2-4x greater floating point and bandwidth peak for the watts</p>
<ul>
<li><p>also for the $ if you buy enterprise gear</p></li>
<li><p>better for the $ if you buy gaming gear</p></li>
</ul>
</li>
<li><p>Step 1 is to assess workload and latency requirements</p></li>
</ul>
<p><img alt="VecDot CPU vs GPU size on the x-axis" src="../_images/VecDot_CPU_vs_GPU_size.png" />
<img alt="VecDot CPU vs GPU size on the x-axis" src="../_images/VecDot_CPU_vs_GPU_time.png" /></p>
<ul class="simple">
<li><p>Don’t waste time with GPUs if</p>
<ul>
<li><p>your problem size or time to solution requirements don’t align</p></li>
<li><p>if the work you’d like to move to the GPU is not a bottleneck</p></li>
<li><p>if the computation cost will be dwarfed by moving data to/from the GPU</p>
<ul>
<li><p>often you need to restructure so that caller passes in data already on the device</p></li>
<li><p>can require nonlocal refactoring</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Almost never: pick one kernel at a time and move it to the GPU</p>
<ul>
<li><p>Real-world examples: DOE ACME/E3SM projects (to pick on one high-profile application) has basically done this for five years and it still doesn’t help their production workloads so they bought a non-GPU machine</p></li>
</ul>
</li>
</ul>
<section id="okay-okay-okay-what-if-i-have-the-right-workload">
<h3>Okay, okay, okay.  What if I have the right workload?<a class="headerlink" href="#okay-okay-okay-what-if-i-have-the-right-workload" title="Link to this heading">#</a></h3>
<section id="terminology-intro">
<h4>Terminology/Intro<a class="headerlink" href="#terminology-intro" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://devblogs.nvidia.com/even-easier-introduction-cuda/">An even easier introduction to CUDA</a></p></li>
<li><p><a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model">CUDA Programming Model</a></p></li>
<li><p>On the CPU, we have a thread with vector registers/instructions</p></li>
<li><p>In CUDA, we write code inside a single vector lane (“confusingly” called a CUDA thread)</p></li>
<li><p>To get inside the lane, we launch a <strong>kernel</strong> from the CPU using special syntax. For example:</p></li>
</ul>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">add</span><span class="o">&lt;&lt;&lt;</span><span class="n">numBlocks</span><span class="p">,</span><span class="w"> </span><span class="n">blockSize</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">);</span>
</pre></div>
</div>
<ul class="simple">
<li><p>needs to be compiled using <code class="docutils literal notranslate"><span class="pre">nvcc</span></code> compiler</p></li>
<li><p>Logically 1D/2D/3D rectangular tiled iteration space</p></li>
</ul>
<p><img alt="CUDA: grid of thread blocks" src="../_images/grid-of-thread-blocks.png" /></p>
<ul class="simple">
<li><p>There are <a class="reference external" href="https://en.wikipedia.org/wiki/CUDA#Version_features_and_specifications">many</a> constraints and limitations to the iteration “grid”</p></li>
</ul>
<p><img alt="CUDA constraints" src="../_images/cuda-constraints.png" /></p>
<ul class="simple">
<li><p>Control flow for CUDA threads is nominally independent, but performance will be poor if you don’t coordinate threads within each block.</p>
<ul>
<li><p>Implicit coordination:</p>
<ul>
<li><p>Memory coalescing</p></li>
<li><p>Organize your algorithm to limit “divergence”</p></li>
</ul>
</li>
<li><p>Explicit coordination:</p>
<ul>
<li><p>Shared memory</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">__syncthreads()</span></code></p></li>
<li><p>Warp shuffles</p></li>
</ul>
</li>
</ul>
</li>
<li><p>We implement the kernel by using the <code class="docutils literal notranslate"><span class="pre">__global__</span></code> attribute</p>
<ul>
<li><p>Visible from the CPU</p></li>
<li><p>Special <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#built-in-variables">built-in variables</a> are defined</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">gridDim</span></code>: dimension of the grid</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">blockIdx</span></code>: block index within the grid</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">blockDim</span></code>: dimensions of the block</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">threadIdx</span></code>: thread index within the block.</p></li>
</ul>
</li>
<li><p>There is also <code class="docutils literal notranslate"><span class="pre">__device__</span></code>, which is callable from other device functions</p></li>
<li><p>Can use <code class="docutils literal notranslate"><span class="pre">__host__</span> <span class="pre">__device__</span></code> to compile two versions</p></li>
</ul>
</li>
</ul>
<p><img alt="CUDA indexing" src="../_images/cuda_indexing.png" /></p>
</section>
<section id="how-does-this-relate-to-the-hardware">
<h4>How does this relate to the hardware?<a class="headerlink" href="#how-does-this-relate-to-the-hardware" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Each thread block is assigned to one <strong>streaming multiprocessor (SM)</strong></p></li>
<li><p>Executed in warps (number of hardware lanes)</p></li>
<li><p>Multiple warps (from the same or different thread blocks) execute like “hyperthreads”</p></li>
</ul>
</section>
</section>
</section>
<section id="id1">
<h2>2. Practical CUDA<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<section id="cuda-best-practices-guide">
<h3><a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html">CUDA Best Practices Guide</a><a class="headerlink" href="#cuda-best-practices-guide" title="Link to this heading">#</a></h3>
<section id="occupancy">
<h4>Occupancy<a class="headerlink" href="#occupancy" title="Link to this heading">#</a></h4>
<blockquote>
<div><p>Thread instructions are executed sequentially in CUDA, and, as a result, executing other warps when one warp is paused or stalled is the only way to hide latencies and keep the hardware busy. Some metric related to the number of active warps on a multiprocessor is therefore important in <strong>determining how effectively the hardware is kept busy</strong>. This metric is <em>occupancy</em>.  [emphasis added]</p>
</div></blockquote>
<ul class="simple">
<li><p>Reality: occupancy is just one aspect, and often inversely correlated with keeping the hardware busy (and with performance).</p></li>
</ul>
<blockquote>
<div><p>Occupancy is the ratio of the number of active warps per multiprocessor to the maximum number of possible active warps.</p>
</div></blockquote>
<ul class="simple">
<li><p>If your kernel uses fewer registers/less shared memory, more warps can be scheduled.</p></li>
<li><p>Register/shared memory usage is determined by the compiler.</p></li>
</ul>
<p>Code example:</p>
<div class="highlight-cuda notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="kr">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">add</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="linenos">2</span><span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nb">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nb">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="linenos">3</span><span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">stride</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nb">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="linenos">4</span><span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">index</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">stride</span><span class="p">)</span>
<span class="linenos">5</span><span class="w">    </span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="linenos">6</span><span class="p">}</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span><span class="w"> </span>nvcc<span class="w"> </span>-c<span class="w"> </span>../cuda_codes/module7-3/add.cu<span class="w"> </span>--resource-usage
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/bin/bash: line 1: nvcc: command not found
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>This shows us <a class="reference external" href="https://docs.nvidia.com/cuda/parallel-thread-execution/">PTX</a> information, where <strong>PTX</strong> is a low-level parallel thread execution virtual machine and instruction set architecture (ISA). PTX exposes the GPU as a data-parallel computing device.</p></li>
<li><p><a class="reference external" href="https://docs.nvidia.com/cuda/parallel-thread-execution/#syntax"><code class="docutils literal notranslate"><span class="pre">ptxas</span></code></a>: PTX programs are a collection of text source modules (files). PTX source modules have an assembly-language style syntax with instruction operation codes and operands. Pseudo-operations specify symbol and addressing management. The <code class="docutils literal notranslate"><span class="pre">ptxas</span></code> optimizing backend compiler optimizes and assembles PTX source modules to produce corresponding binary object files.</p></li>
</ul>
<p><strong>Understanding the <code class="docutils literal notranslate"><span class="pre">ptxas</span> <span class="pre">info</span></code></strong>:</p>
<ul class="simple">
<li><p>Find the documentation page <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#printing-code-generation-statistics">here</a>.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">gmem</span></code>: global memory</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">stack</span> <span class="pre">frame</span></code> is the per thread stack usage used by this function.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">spill</span> <span class="pre">stores</span></code> and <code class="docutils literal notranslate"><span class="pre">spill</span> <span class="pre">loads</span></code> represent stores and loads done on stack memory which are being used for storing variables that couldn’t be allocated to physical registers.</p></li>
<li><p>Number of used <code class="docutils literal notranslate"><span class="pre">registers</span></code> is pretty self-explanatory</p></li>
<li><p>And amount of total space allocated in constant bank, <code class="docutils literal notranslate"><span class="pre">cmem</span></code>, is shown.</p></li>
<li><p>On more modern versions, also amount of shared memory, <code class="docutils literal notranslate"><span class="pre">smem</span></code>, can be shown.</p></li>
</ul>
</li>
</ul>
<p>Remarks:</p>
<ul class="simple">
<li><p>A stack is not a concept that is unique or specific to CUDA. A stack frame is simply the space utilized by the stack, or the space utilized to conduct a particular operation on/with the stack (such as a function or subroutine call).</p></li>
<li><p>Spill stores and spill loads relate to the usage of variables in the logical local space. Such variables may manifest in a register, or they may manifest in DRAM memory, or perhaps the caches.</p>
<ul>
<li><p>The GPU is a load/store architecture for the most part, so when it comes time to use variables in calculations, they almost universally manifest in GPU registers.</p></li>
</ul>
</li>
</ul>
<div class="highlight-cuda notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="kr">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">copy</span><span class="p">(</span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">dst</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">src</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="linenos">2</span><span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">iblock</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nb">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nb">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="linenos">3</span><span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">index</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="nb">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">TILE_SIZE</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">iblock</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nb">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="linenos">4</span><span class="w">  </span><span class="kt">float</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">TILE_SIZE</span><span class="p">];</span><span class="w"> </span><span class="c1">// allocated in registers</span>
<span class="linenos">5</span><span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">&lt;</span><span class="n">TILE_SIZE</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span>
<span class="linenos">6</span><span class="w">    </span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">src</span><span class="p">[</span><span class="n">index</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nb">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
<span class="linenos">7</span><span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">&lt;</span><span class="n">TILE_SIZE</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span>
<span class="linenos">8</span><span class="w">    </span><span class="n">dst</span><span class="p">[</span><span class="n">index</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nb">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="linenos">9</span><span class="p">}</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span><span class="w"> </span>nvcc<span class="w"> </span>-c<span class="w"> </span>../cuda_codes/module7-3/copy.cu<span class="w"> </span>--resource-usage<span class="w"> </span>-DTILE_SIZE<span class="o">=</span><span class="m">16</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/bin/bash: line 1: nvcc: command not found
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>The <a class="reference external" href="https://docs.nvidia.com/nsight-compute/NsightCompute/index.html#occupancy-calculator">NVIDIA Nsight Occupancy Calculator</a> can compute occupancy based on the register and shared memory usage.</p></li>
<li><p>You can tell the compiler to reduce register usage, sometimes at the expense of spills.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span><span class="w"> </span>nvcc<span class="w"> </span>-c<span class="w"> </span>../cuda_codes/module7-3/copy.cu<span class="w"> </span>--resource-usage<span class="w"> </span>-DTILE_SIZE<span class="o">=</span><span class="m">16</span><span class="w"> </span>--maxrregcount<span class="w"> </span><span class="m">24</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/bin/bash: line 1: nvcc: command not found
</pre></div>
</div>
</div>
</div>
<p>In the example above, we have used the <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html?highlight=maxrregcount#maxrregcount-amount-maxrregcount"><code class="docutils literal notranslate"><span class="pre">--maxrregcount</span></code></a> compiler flag that specifies the maximum amount of registers that GPU functions can use.</p>
<blockquote>
<div><p>Until a function-specific limit, a higher value will generally increase the performance of individual GPU threads that execute this function. However, because thread registers are allocated from a global register pool on each GPU, a higher value of this option will also reduce the maximum thread block size, thereby reducing the amount of thread parallelism. Hence, a good <code class="docutils literal notranslate"><span class="pre">maxrregcount</span></code> value is the result of a <em>trade-off</em>.</p>
</div></blockquote>
</section>
<section id="further-reading">
<h4>Further reading:<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Vasily Volkov (2010) <a class="reference external" href="https://www.nvidia.com/content/GTC-2010/pdfs/2238_GTC2010.pdf"><strong>Better Performance at Lower Occupancy</strong></a> (slides)</p></li>
<li><p>Vasily Volkov (2016) <a class="reference external" href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-143.pdf"><strong>Understanding Latency Hiding on GPUs</strong></a> (very in-depth)</p></li>
<li><p>Kasia Swirydowicz (2018) <a class="reference external" href="https://www.paranumal.com/single-post/2018/03/02/Finite-Element-Stiffness-Matrix-Action-monolithic-kernel-optimization-on-Titan-V"><strong>Finite Element Stiffness Matrix Action: monolithic kernel optimization on Titan V</strong></a></p></li>
</ul>
</section>
</section>
</section>
<section id="memory">
<h2>3. Memory<a class="headerlink" href="#memory" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>GPU memory is <em>not</em> CPU memory</p></li>
</ul>
<p><img alt="A single socket of the Summit supercomputer" src="https://en.wikichip.org/w/images/4/47/summit_single-socket.svg" /></p>
<p><strong>Duh</strong>, so why does NVIDIA <a class="reference external" href="https://devblogs.nvidia.com/unified-memory-cuda-beginners/">publish this</a>?</p>
<p><img alt="Image from nvidia.com Unified Memory for CUDA Beginners page" src="https://devblogs.nvidia.com/wp-content/uploads/2017/06/Unified-Memory-MultiGPU.png" /></p>
<p><strong>Getting your memory into position is often the hardest part of CUDA programming.</strong></p>
<p>You need to:</p>
<ul class="simple">
<li><p>Allocate memory on the GPU:</p></li>
</ul>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">xdevice</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">));</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Populate it from the host:</p></li>
</ul>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">xdevice</span><span class="p">,</span><span class="w"> </span><span class="n">xhost</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">),</span><span class="w"> </span><span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Repeat for all data, including control parameters</p></li>
<li><p>Easy to forget, ongoing maintenance/complexity cost</p></li>
</ul>
<section id="unified-managed-memory">
<h3><a class="reference external" href="https://devblogs.nvidia.com/unified-memory-cuda-beginners/">Unified/managed memory</a><a class="headerlink" href="#unified-managed-memory" title="Link to this heading">#</a></h3>
<blockquote>
<div><p>This hardware/software technology allows applications to allocate data that can be read or written from code running on either CPUs or GPUs. Allocating Unified Memory is as simple as replacing calls to <code class="docutils literal notranslate"><span class="pre">malloc()</span></code> or <code class="docutils literal notranslate"><span class="pre">new</span></code> with calls to <code class="docutils literal notranslate"><span class="pre">cudaMallocManaged()</span></code>, an allocation function that returns a pointer accessible from any processor.</p>
</div></blockquote>
<ul class="simple">
<li><p>Allocate “managed” memory, accessible from CPU and GPU:</p></li>
</ul>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
</pre></div>
</div>
<ul class="simple">
<li><p>How?</p></li>
</ul>
<p><img alt="Maximizing Unified Memory" src="../_images/maximizing-unified-memory.png" /></p>
<ul class="simple">
<li><p>With OpenACC, you can make all dynamic allocations in managed memory. Example: <code class="docutils literal notranslate"><span class="pre">pgcc</span> <span class="pre">-ta=tesla:managed</span></code></p>
<ul>
<li><p>The GPU probably has less memory than you have DRAM</p></li>
<li><p>Really convenient for incremental work in legacy code</p></li>
<li><p>Performance isn’t great without <code class="docutils literal notranslate"><span class="pre">cudaMemPrefetchAsync</span></code></p></li>
</ul>
</li>
</ul>
<p><img alt="Streaming performance unified memory GPU" src="../_images/streaming_performance_unified_memory_gpu.png" /></p>
<p><strong>Further reading</strong>: <a class="reference external" href="https://devblogs.nvidia.com/maximizing-unified-memory-performance-cuda/">Maximizing Unified Memory Performance in CUDA</a></p>
</section>
<section id="on-memory-coalescing-and-strided-access">
<h3>3.1 On memory coalescing and <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#strided-accesses">strided access</a><a class="headerlink" href="#on-memory-coalescing-and-strided-access" title="Link to this heading">#</a></h3>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">strideCopy</span><span class="p">(</span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">odata</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">idata</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">stride</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">xid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="n">stride</span><span class="p">;</span>
<span class="w">    </span><span class="n">odata</span><span class="p">[</span><span class="n">xid</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">idata</span><span class="p">[</span><span class="n">xid</span><span class="p">];</span>
<span class="p">}</span>
</pre></div>
</div>
<p>This kernel copies data with a stride of <code class="docutils literal notranslate"><span class="pre">stride</span></code> elements between threads from <code class="docutils literal notranslate"><span class="pre">idata</span></code> to <code class="docutils literal notranslate"><span class="pre">odata</span></code>.</p>
<blockquote>
<div><p>ensuring that as much as possible of the data in each cache line fetched is actually used is an important part of performance optimization of memory accesses on these devices.</p>
</div></blockquote>
<p><img alt="" src="../_images/adjacent-threads-accessing-memory-with-stride-of-2.png" /></p>
<p>In this case, threads within a warp access words in memory with a stride of 2. This action leads to a load of eight L2 cache segments per warp on a Tesla V100 (compute capability 7.0).</p>
<p>We lose half of the bandwidth for <code class="docutils literal notranslate"><span class="pre">stride=2</span></code>:</p>
<p>In fact, a stride of 2 results in a 50% of load/store efficiency since half the elements in the transaction are not used and represent wasted bandwidth.</p>
<p>As the stride increases, the effective bandwidth decreases until the point where 32 32-byte segments are loaded for the 32 threads in a warp.</p>
<p>We can do better by aligning the memory loads, stride size and warp size.</p>
<p><img alt="nvidia.com website performance of stridecopy kernel" src="../_images/performance-of-stridecopy-kernel.png" /></p>
</section>
</section>
<section id="tuckoo-demo-for-cuda-codes">
<h2>4. Tuckoo demo for CUDA codes<a class="headerlink" href="#tuckoo-demo-for-cuda-codes" title="Link to this heading">#</a></h2>
<p>The Tuckoo cluster is equipped with four NVIDIA P100s. To be able to compile and execute CUDA code on one of the GPU nodes, we first need to log in on an <em>interactive node</em>.</p>
<p>An <em>interactive node</em> on a cluster is often a node that is dedicated to quick compilation/testing/debugging of your codes-<strong>not</strong> to high-demanding or large-scale computations-. If you misuse an interactive node, a cluster admin might kick you out of that node.</p>
<ul class="simple">
<li><p>For tuckoo, <strong><code class="docutils literal notranslate"><span class="pre">node8</span></code></strong> is the node dedicated for interactive usage, where you can find the <code class="docutils literal notranslate"><span class="pre">nvcc</span></code> compiler to compile CUDA codes.</p></li>
</ul>
<p>Once you are logged in on tuckoo using</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>ssh<span class="w"> </span>your_user_name@tuckoo.sdsu.edu
</pre></div>
</div>
<p>You will see that you are logged in on the <em>login node</em> because the prompt will show</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>your_user_name@tuckoo<span class="w"> </span>~<span class="o">]</span>$
</pre></div>
</div>
<p>Now you can use <code class="docutils literal notranslate"><span class="pre">rsh</span></code> to connect to <code class="docutils literal notranslate"><span class="pre">node8</span></code> by simply running:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>rsh<span class="w"> </span>node8
</pre></div>
</div>
<p>and you will see that you are indeed on <code class="docutils literal notranslate"><span class="pre">node8</span></code> because the prompt has changed to:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>your_user_name@node8<span class="w"> </span>~<span class="o">]</span>$
</pre></div>
</div>
<p>Now you can compile any CUDA code by invoking the <code class="docutils literal notranslate"><span class="pre">nvcc</span></code> compiler and execute it by sending it to the Slurm batch scheduler via a batch script.</p>
<p>You can find an example of a batch script to execute our <code class="docutils literal notranslate"><span class="pre">./hello_world</span></code> cuda code in <a class="reference external" href="https://github.com/sdsu-comp605/spring25/tree/main/batch_scripts/batch-cuda"><code class="docutils literal notranslate"><span class="pre">batch_scripts/batch-cuda</span></code></a>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/bin/sh</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="c1">#note: run cuda hello-world on a single node, using a GPU</span>
<span class="linenos"> 4</span>
<span class="linenos"> 5</span><span class="c1">#SBATCH --job-name=cuhello</span>
<span class="linenos"> 6</span><span class="c1">#SBATCH --output=%A-cuhello.out</span>
<span class="linenos"> 7</span><span class="c1">#SBATCH --ntasks=16</span>
<span class="linenos"> 8</span><span class="c1">#SBATCH --constraint=P100</span>
<span class="linenos"> 9</span>
<span class="linenos">10</span><span class="nb">export</span><span class="w"> </span><span class="nv">OMPI_MCA_pml</span><span class="o">=</span>ob1
<span class="linenos">11</span><span class="nb">export</span><span class="w"> </span><span class="nv">OMPI_MCA_btl</span><span class="o">=</span>tcp,self
<span class="linenos">12</span>
<span class="linenos">13</span><span class="c1">#require tcp over infiniband</span>
<span class="linenos">14</span><span class="nb">export</span><span class="w"> </span>OMPI_MCA_btl_tcp_if_include<span class="w"> </span>ib0
<span class="linenos">15</span>
<span class="linenos">16</span>./hello_cuda
<span class="linenos">17</span>
<span class="linenos">18</span><span class="c1">#-------------------------------------------------------------</span>
</pre></div>
</div>
<p>Notice the</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --constraint=P100</span>
</pre></div>
</div>
<p>directive that tells Slurm to execute this code on one of the available compute nodes equipped with the P100s.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "sdsu-comp605/spring25",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="module7-2_cuda.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">24) GPUs and CUDA</p>
      </div>
    </a>
    <a class="right-next"
       href="module8-1_cuda_jl.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">26) Intro to GPU programming in Julia</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-use-a-gpu">1. When to use a GPU?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#okay-okay-okay-what-if-i-have-the-right-workload">Okay, okay, okay.  What if I have the right workload?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#terminology-intro">Terminology/Intro</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-this-relate-to-the-hardware">How does this relate to the hardware?</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">2. Practical CUDA</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cuda-best-practices-guide">CUDA Best Practices Guide</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#occupancy">Occupancy</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further reading:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#memory">3. Memory</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unified-managed-memory">Unified/managed memory</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#on-memory-coalescing-and-strided-access">3.1 On memory coalescing and strided access</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tuckoo-demo-for-cuda-codes">4. Tuckoo demo for CUDA codes</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Valeria Barra
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>