{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21) Introduction to MPI.jl\n",
    "\n",
    "Last time:\n",
    "- Slurm demo\n",
    "- Parallel linear algebra\n",
    "\n",
    "Today:\n",
    "1. Introduction to MPI.jl\n",
    "2. Setup MPI.jl\n",
    "3. Hello world\n",
    "4. Setup on tuckoo cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to MPI.jl\n",
    "\n",
    "Recap on MPI so far:\n",
    "\n",
    "- Each process executes its own program with its own data (**MPMD: Multiple Program, Multiple Data**)\n",
    "- Each process can communicate with every other process through messages (chunks of binary data, generally arrays of primitive data types like `int`, `double`, etc..)\n",
    "- Messages can be _point-to-point_ (between only two processes) or _collective_ such as _broadcasts_ (one to all) and _reductions_ (all to one).\n",
    "- Though in principle each process can execute a unique program, generally each process executes the same program but the work it does depends on it's unique number within all the processes.\n",
    "\n",
    "**Terminology**:\n",
    "\n",
    "- **communicator**: a group of processes that can communicate (a process can be in many different communicators, though in this class we will generally just use a single global communicator).\n",
    "\n",
    "- **rank**: the unique identifying number of a process within a communicator. Process ranks run from 0 to the number of processes minus one in the communicator. In general, a processes rank will have a different in each communicator it is a member of.\n",
    "\n",
    "[MPI.jl](https://juliaparallel.org/MPI.jl/stable/) is a basic Julia wrapper for the MPI standard. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup MPI.jl\n",
    "\n",
    "To be able to run MPI instructions in Julia, we first need to [configure MPI.jl](https://juliaparallel.org/MPI.jl/stable/configuration/).\n",
    "\n",
    "To do so, we need the auxiliary package MPIPreferences.jl. This will (hopefully) detect an MPI implementation library on your system at one of the default locations. \n",
    "\n",
    "To do so, launch Julia and in the REPL do the following:\n",
    "\n",
    "```bash\n",
    "julia\n",
    "```\n",
    "\n",
    "```julia\n",
    "julia> using Pkg; Pkg.add(\"MPIPreferences.jl\")\n",
    "\n",
    "julia> using MPIPreferences; MPIPreferences.use_system_binary()\n",
    "```\n",
    "\n",
    "The call to `use_system_binary()` should automatically detect the MPI binary. \n",
    "\n",
    "Now you should be ready to run your first MPI.jl hello world program.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hello world\n",
    "\n",
    "In the Julia REPL, you can do:\n",
    "\n",
    "```julia\n",
    "julia> using Pkg; Pkg.add(\"MPI.jl\")\n",
    "\n",
    "julia> using MPI; mpiexec(cmd -> run(`$cmd -n 3 echo hello world`))\n",
    "```\n",
    "\n",
    "Or you can find the following example in [`julia_codes/module6-1/`](https://github.com/sdsu-comp605/spring25/tree/main/julia_codes/module6-1/).\n",
    "\n",
    "\n",
    "```{literalinclude} ../julia_codes/module6-1/mpi_hello_world.jl\n",
    ":language: julia\n",
    ":linenos: true\n",
    "```\n",
    "\n",
    "Once you have setup your environment via MPIPreferences and added MPI, you can run the above example under your `mpiexec`:\n",
    "\n",
    "\n",
    "```bash\n",
    "mpiexec -n 4 julia ./julia_codes/module6-1/mpi_hello_world.jl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup on tuckoo cluster\n",
    "\n",
    "On the cluster, you can simply start julia by typing `julia` from any location. \n",
    "\n",
    "We will still use MPIPreferences to set up our environment, but this time the call to `use_system_binary()` instead of having an empty default list of arguments, will include the argument `library_names` with the location of where the MPI binary resides on the parallel file system in the cluster.\n",
    "\n",
    "```\n",
    "$ julia\n",
    "\n",
    "julia> using Pkg; Pkg.add(\"MPIPreferences\")\n",
    "julia> using MPIPreferences\n",
    "julia> MPIPreferences.use_system_binary(library_names=\"/usr/lib64/openmpi/lib/libmpi.so\")\n",
    "julia> using Pkg; Pkg.add(\"MPI\")\n",
    "```\n",
    "\n",
    "Once your environment is setup on the cluster, you can run with \n",
    "\n",
    "```bash\n",
    "mpiexec -np 4 julia mpi_hello_world.jl\n",
    "```\n",
    "\n",
    "You can find an example of a batch script that launches a julia job in `/examples/slurm/batch-jello`\n",
    "\n",
    "```bash\n",
    "#!/bin/sh\n",
    "\n",
    "#note: run julia hello-world on a single node\n",
    "\n",
    "#SBATCH --job-name=jhello\n",
    "#SBATCH --output=%A-jhello.out\n",
    "#SBATCH --ntasks=16\n",
    "\n",
    "export OMPI_MCA_pml=ob1\n",
    "export OMPI_MCA_btl=tcp,self\n",
    "\n",
    "#require tcp over infiniband\n",
    "export OMPI_MCA_btl_tcp_if_include ib0\n",
    "\n",
    "mpirun julia mpi_hello_world.jl\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
