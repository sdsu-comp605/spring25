{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19) Introduction to Batch Jobs\n",
    "\n",
    "Last time:\n",
    "- Introduction to MPI\n",
    "\n",
    "Today: \n",
    "1. [Introduction to Batch Jobs and Job Scripting](#introduction-to-batch-jobs-and-job-scripting) \n",
    "2. [SLURM Demo](#slurm-demo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Batch Jobs and Job Scripting\n",
    "\n",
    "* Batch jobs are, by far, the most common type of job on HPC systems.\n",
    "\n",
    "* Batch jobs are resource provisions that run applications on compute nodes and do not require supervision or interaction. \n",
    "\n",
    "* Batch jobs are commonly used for applications that run for long periods of time or require little to no user input.\n",
    "\n",
    "* To submit jobs that use multiple resources in parallel, you need a [job/task scheduling system](https://en.wikipedia.org/wiki/Parallel_task_scheduling).\n",
    "  - SLURM is a very popular parallel job scheduler (although not the only one).\n",
    "  - SLURM submits/manages jobs to its own scheduling system via the [`sbatch`](https://slurm.schedmd.com/sbatch.html) command.\n",
    "\n",
    "### Jobs scripting\n",
    "\n",
    "Even though it is possible to run jobs completely from the command line, it is often overly tedious and unorganized to do so. \n",
    "\n",
    "Instead, it is recommended to construct a **job script** for your batch jobs. \n",
    "\n",
    "- A job script is a set of Linux commands paired with a set of resource requirements that can be passed to the Slurm job scheduler. \n",
    "- Slurm will then generate a job according to the parameters set in the job script. \n",
    "- Any commands that are included with the job script will be run within the job.\n",
    "\n",
    "### Running a job script\n",
    "\n",
    "Running a job script can be done with the `sbatch` command:\n",
    "\n",
    "```bash\n",
    "sbatch <your-job-script-name>\n",
    "```\n",
    "\n",
    "Once you submit yout batch job, you may see the following message:\n",
    "\n",
    "```bash\n",
    "Submitted batch job <job_ID>\n",
    "```\n",
    "\n",
    "You can query the status of your job (wait in queue/running/ended, etc) with:\n",
    "\n",
    "```bash\n",
    "squeue -u your_user_name\n",
    "```\n",
    "\n",
    "### Making a job script\n",
    "\n",
    "A job script looks something like this:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --time=00:10:00\n",
    "#SBATCH --partition=<any_needed_specific_partition>\n",
    "#SBATCH --output=sample-%j.out\n",
    "\n",
    "module purge\n",
    "\n",
    "module load <necessary_modules_to_be_loaded> # such as compilers needed etc\n",
    "\n",
    "echo \"== This is the scripting step! ==\"\n",
    "sleep 30\n",
    "./executable\n",
    "echo \"== End of Job ==\"\n",
    "```\n",
    "\n",
    "Normally job scripts are divided into 3 primary parts: \n",
    "* directives\n",
    "* loading software\n",
    "* user scripting. \n",
    "\n",
    "Directives give the terminal and the Slurm daemon instructions on setting up the job. \n",
    "\n",
    "Loading software involves cleaning out the environment and loading specific pieces of software you need for your job. \n",
    "\n",
    "User scripting is simply the commands you wish to be executed in your job.\n",
    "\n",
    "#### Directives\n",
    "\n",
    "The first directive, the `shebang` directive, is always on the first line of any script. \n",
    "\n",
    "The directive indicates which shell you want running commands in your job. \n",
    "\n",
    "Most users employ bash as their shell, so we will specify bash by typing:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "```\n",
    "\n",
    "The next directives that must be included with your job script are _sbatch_ directives. \n",
    "\n",
    "These directives specify resource requirements to Slurm for a batch job. \n",
    "\n",
    "These directives must come _after_ the _shebang_ directive and before any commands are issued in the job script. \n",
    "\n",
    "Each directive contains a flag that requests a resource the job would need to complete execution. \n",
    "\n",
    "You can also send yourself an email to alert you that your job has ended.\n",
    "\n",
    "An sbatch directive is written as such:\n",
    "\n",
    "```bash\n",
    "#SBATCH --<resource>=<amount>\n",
    "```\n",
    "\n",
    "For example, if you wanted to request 2 nodes with an sbatch directive, you would write:\n",
    "\n",
    "```bash\n",
    "#SBATCH --nodes=2\n",
    "```\n",
    "\n",
    "Other useful directives:\n",
    "* `--ntasks=processes` to specify **total** number of tasks\n",
    "* `--ntasks-per-node=processes` to specify the number of processes you wish to assign to each node\n",
    "* `--mem=memory` sets the **total memory** (per node requested) required for the job.\n",
    "\n",
    "Having more resources (e.g., they can be subdivided in \"Resource Sets\" etc.) means having more parameters to specify! For instance, researchers at ORNL, put together workshops on [How to submit jobs on the Summit supercomputer](https://www.olcf.ornl.gov/wp-content/uploads/2022/02/Introduction-to-Job-Submission-on-Summit.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SLURM Demo\n",
    "\n",
    "You can find a couple of batch job scripts in [`batch_scripts/`](https://github.com/sdsu-comp605/spring25/tree/main/batch_scripts) that you can execute via `sbatch` on the cluster."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
