{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "072ffef1",
   "metadata": {},
   "source": [
    "# 30) Parallel reductions with CUDA.jl\n",
    "\n",
    "Last time:\n",
    "- Memory management with CUDA.jl\n",
    "- Simple kernels using global memory  \n",
    "- Simple kernels using shared/local memory  \n",
    "- Instruction Level Parallelism  \n",
    "-  Bank Conflicts\n",
    "\n",
    "Today:\n",
    "\n",
    "1. Outline\n",
    "2. Parallel reduction on the GPU\n",
    "3. Parallel reduction complexity\n",
    "4. Different strategies for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b9a2a3",
   "metadata": {},
   "source": [
    "## 1. Outline\n",
    "\n",
    "In this lecture, we will use an efficient parallel reduction on the GPU as an example to talk about:\n",
    "\n",
    "- communication across thread blocks\n",
    "- global synchronization problem across thread blocks\n",
    "- use of different memory pools for optimization strategies\n",
    "\n",
    ":::{note} References:\n",
    "- We will losely follow [Optimizing Parallel Reduction in CUDA](http://developer.download.nvidia.com/assets/cuda/files/reduction.pdf) by Mark Harris. Note that some of our kernels will be different than those given in the talk, but the ideas are (mostly) the same.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d0b9f7",
   "metadata": {},
   "source": [
    "## 2. Parallel reduction on the GPU\n",
    "\n",
    "We want to perform a Parallel Reduction operation on an array of size $N$. \n",
    "\n",
    "This is a common and important data parallel primitive. It should be:\n",
    "\n",
    "- Easy to implement in CUDA\n",
    "- But often harder to get it right\n",
    "- We can use this as a great optimization example\n",
    "\n",
    "### Basic idea\n",
    "\n",
    "- Tree-based approach used within each thread block\n",
    "\n",
    "![A tree-based approach for a parallel reduction](../img/tree_based_parallel_reduction.png \"A tree-based approach for a parallel reduction\")\n",
    "\n",
    "- Need to be able to use multiple thread blocks\n",
    "  * to process very large arrays\n",
    "  * to keep all multiprocessors on the GPU busy\n",
    "  * each thread block reduces a portion of the array\n",
    "- But how do we communicate partial results between\n",
    "thread blocks?\n",
    "\n",
    "### Problem: Global Synchronization\n",
    "\n",
    "- If we could synchronize across all thread blocks, we _could_ easily reduce very large arrays:\n",
    "  * global sync after each block would produce its result\n",
    "  * once all blocks reach sync, we would continue recursively\n",
    "- But **CUDA has no global synchronization.** Why?\n",
    "  * It's expensive to build in hardware for GPUs with high processor count\n",
    "  * It would force programmers to run fewer blocks (no more than # multiprocessors $\\times$ # resident blocks / multiprocessor) to avoid deadlock, which may reduce overall efficiency \n",
    "\n",
    "**Solution**:  decompose into multiple kernels.\n",
    "\n",
    "- Kernel launch serves as a global synchronization point\n",
    "- Kernel launch has negligible hardware overhead and low software overhead\n",
    "\n",
    "### Solution: Kernel Decomposition\n",
    "\n",
    "- We avoid global sync by decomposing the computation into multiple kernel invocations\n",
    "\n",
    "\n",
    "![A kernel decomposition for a parallel reduction on the GPU](../img/kernel_decomposition_for_reduction.png \"A kernel decomposition for a parallel reduction on the GPU\")\n",
    "\n",
    "- In the case of reductions, the code for all levels is the\n",
    "same: \n",
    "  * we have _recursive_ kernel invocations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf9ed3b",
   "metadata": {},
   "source": [
    "### Measuring success: what are we striving for?\n",
    "\n",
    "- We should strive to reach GPU peak performance\n",
    "\n",
    "- But we need to choose the right metric to measure success:\n",
    "  * GFLOP/s: for compute-bound kernels\n",
    "  * Bandwidth: for memory-bound kernels\n",
    "\n",
    "- Reductions have very low arithmetic intensity\n",
    "  * 1 flop per element loaded (bandwidth-optimal)\n",
    "- Therefore we are _not_ compute-bound, hence we should strive for peak bandwidth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb4e187",
   "metadata": {},
   "source": [
    "## 3. Parallel reduction complexity\n",
    "\n",
    "- $\\log(N)$ parallel steps, each step $S$ does $N/2^S$ independent operations\n",
    "    * _Step complexity_ is $O(\\log N)$\n",
    "- For $N=2^D$ (we are sticking to power-of-2 block sizes), performs $\\Sigma_{S \\in [1, \\dots, D]} 2^{D-S} = N-1$ operations\n",
    "    * _Work complexity_ is $O(N)$ - It is _work-efficient_, i.e., does not perform more operations than a sequential algorithm \n",
    "- With $P$ threads physically in parallel ($P$ processors), _time complexity_ is $O(\\frac{N}{P} + \\log N)$\n",
    "    * Compare to $O(N)$ for sequential reduction\n",
    "    * In a thread block, $N=P$, so $O(\\log N)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7389e1",
   "metadata": {},
   "source": [
    "## 4. Different strategies for optimization\n",
    "\n",
    "The code for the Julia GPU reduction can be found in [julia_codes/module8-3/reduction.jl](https://github.com/sdsu-comp605/spring25/blob/main/julia_codes/module8-3/reduction.jl)\n",
    "\n",
    "### Basic implementation idea:\n",
    "\n",
    "#### Version 1\n",
    "\n",
    "Because there is no global synchronization, we need to launch the kernel multiple times in _thread blocks_ (work groups). \n",
    "\n",
    "In each launch, each thread block will work together to reduce the set of numbers to a single summed value.\n",
    "\n",
    "- In the first iteration we launch, say, `num_groups` (8 by default in the kernel below) work groups to reduce a vector of length `N` to a vector of length `num_groups`. \n",
    "\n",
    "- We will then recurse and reduce the vector of length `num_groups` to a smaller vector, and continue until we only have `1` element in the partially reduced vector.\n",
    "\n",
    "\n",
    "![Interleaved reduction on the GPU](../img/interleaved_reduction_gpu.png \"Interleaved reduction on the GPU\")\n",
    "\n",
    "```julia\n",
    "\"\"\"\n",
    "   reduction_knl_v1(x, y; op=+, ldim::Val{LDIM}=Val(256))\n",
    "\n",
    "Basic interleaved memory access version where each `LDIM` chunk of values from `x`\n",
    "are reduced using `op` (+ by default) to single values which are stored in the first\n",
    "`gridDim().x` values of `y`\n",
    "\n",
    "Problems:\n",
    "  - Modulo is slow on the GPU >> Remove by calculating\n",
    "  - threads in a warp are divergent (for the if statement) >> shift threads as algorithm progresses\n",
    "\"\"\"\n",
    "function knl_reduction_v1!(y, x, N, op, ::Val{LDIM}) where LDIM\n",
    "  tid = threadIdx().x\n",
    "  bid = blockIdx().x\n",
    "  gid = tid + (bid - 1) * LDIM # global thread index\n",
    "  l_x = @cuStaticSharedMem(eltype(x), LDIM) # shared memory allocation\n",
    "\n",
    "  @inbounds begin\n",
    "    # set the local/shared memory array\n",
    "    if gid <= N\n",
    "      l_x[tid] = x[gid]\n",
    "    else\n",
    "      l_x[tid] = 0\n",
    "    end\n",
    "\n",
    "    sync_threads()\n",
    "\n",
    "    s = 1\n",
    "    while s < LDIM\n",
    "      # I'm still active if my thread ID (minus 1) is divisible by 2*s\n",
    "      if ((tid-1) % (2 * s) == 0)\n",
    "        # combine my value to my (interleaved/strided) neighbors value\n",
    "        l_x[tid] = op(l_x[tid], l_x[tid + s])\n",
    "      end\n",
    "      s *= 2\n",
    "      sync_threads()\n",
    "    end\n",
    "\n",
    "    # Thread 1 is the only remaining \"real\" thread which will carry the result\n",
    "    tid == 1 && (y[bid] = l_x[tid])\n",
    "  end\n",
    "\n",
    "  nothing\n",
    "end\n",
    "```\n",
    "\n",
    "**Problems**:\n",
    "1. Modulo (`%`) is very slow\n",
    "2. Highly divergent warps are very inefficient. We have an if-statement for which, at the first step, half of the threads in a warp will be idleing; in subsequent steps, even more will be idleing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6e7d9a",
   "metadata": {},
   "source": [
    "#### Version 2\n",
    "\n",
    "- Work-group (block size) sized elements of the given array into shared memory (as in version 1)\n",
    "- Use binary sum to reduce the values to a single number (as in version 1)\n",
    "- Recurse until there is only one group (as in version 1)\n",
    "- BUT: replace divergent branch in inner loop, with strided index and non-divergent branch\n",
    "\n",
    "\n",
    "![Interleaved reduction on the GPU without divergence](../img/interleaved_strided_index_no_divergence.png  \"Interleaved reduction on the GPU without divergence\")\n",
    "\n",
    "```julia\n",
    "\"\"\"\n",
    "   reduction_knl_v2(x, y; op=+, ldim::Val{LDIM}=Val(256))\n",
    "\n",
    "Improved strided memory access version where each `LDIM` chunk of values from `x`\n",
    "are reduced using `op` to single values which are stored in the first\n",
    "`gridDim().x` values of `y`\n",
    "\n",
    "Problems:\n",
    "  - Memory access is still strided, thus bank conflicts >> sequential access\n",
    "\"\"\"\n",
    "function knl_reduction_v2!(y, x, N, op, ::Val{LDIM}) where LDIM\n",
    "  tid = threadIdx().x\n",
    "  bid = blockIdx().x\n",
    "  gid = tid + (bid - 1) * LDIM\n",
    "  l_x = @cuStaticSharedMem(eltype(x), LDIM)\n",
    "\n",
    "  @inbounds begin\n",
    "    if gid <= N\n",
    "      l_x[tid] = x[gid]\n",
    "    else\n",
    "      l_x[tid] = 0\n",
    "    end\n",
    "\n",
    "    sync_threads()\n",
    "\n",
    "    s = 1\n",
    "    while s < LDIM\n",
    "      # figure out whether I'm in the active block\n",
    "      sid = 2 * (tid-1) * s + 1\n",
    "      if sid + s <= LDIM\n",
    "        # combine my value to my (strided) neighbors value\n",
    "        l_x[sid] = op(l_x[sid], l_x[sid+s])\n",
    "      end\n",
    "      s *= 2\n",
    "      sync_threads()\n",
    "    end\n",
    "\n",
    "    # Thread 1 is the only remaining \"real\" thread\n",
    "    tid == 1 && (y[bid] = l_x[tid])\n",
    "  end\n",
    "\n",
    "  nothing\n",
    "end\n",
    "```\n",
    "\n",
    "**Observations**:\n",
    "- Note: In the graphics, version 2 has sequential thread IDs instead of chosing theads based on if they are divisible by $2*s$ (where $s$ is the step number) as in version 1.\n",
    "\n",
    "**Problem**:\n",
    "- New Problem: Shared Memory Bank Conflicts!\n",
    "\n",
    "#### Version 3\n",
    "\n",
    "- Use sequential addressing of main memory\n",
    "- This version with sequential addressing is conflict free (i.e., no bank conflicts)\n",
    "- We replace strided indexing in inner loop, with reversed loop and `threadID`-based indexing\n",
    "\n",
    "\n",
    "![Sequential addressing reduction](../img/sequential_addressing_reduction.png \"Sequential addressing reduction\")\n",
    "\n",
    "```julia\n",
    "\"\"\"\n",
    "   reduction_knl_v3(x, y; op=+, ldim::Val{LDIM}=Val(256))\n",
    "\n",
    "Basic sequential memory access version where each `LDIM` chunk of values from\n",
    "`x` are reduced using `op` to single values which are stored in the first\n",
    "`gridDim().x` values of `y`\n",
    "\n",
    "Problems:\n",
    "  - Some threads do very little work (idle threads) >> have each thread add in a few values\n",
    "\"\"\"\n",
    "function knl_reduction_v3!(y, x, N, op, ::Val{LDIM}) where LDIM\n",
    "  tid = threadIdx().x\n",
    "  bid = blockIdx().x\n",
    "  gid = tid + (bid - 1) * LDIM\n",
    "  l_x = @cuStaticSharedMem(eltype(x), LDIM)\n",
    "\n",
    "  @inbounds begin\n",
    "    if gid <= N\n",
    "      l_x[tid] = x[gid]\n",
    "    else\n",
    "      l_x[tid] = 0\n",
    "    end\n",
    "\n",
    "    sync_threads()\n",
    "\n",
    "    # bit right shift to divide by 2\n",
    "    s = LDIM >> 1\n",
    "    while s > 0\n",
    "      # I'm still active if my thread ID is less than\n",
    "      if tid <= s\n",
    "        # combine my value to my (strided) neighbors value\n",
    "        l_x[tid] = op(l_x[tid], l_x[tid + s])\n",
    "      end\n",
    "      # bit right shift to divide by 2\n",
    "      s = s >> 1\n",
    "      sync_threads()\n",
    "    end\n",
    "\n",
    "    # Thread 1 is the only remaining \"real\" thread\n",
    "    tid == 1 && (y[bid] = l_x[tid])\n",
    "  end\n",
    "\n",
    "  nothing\n",
    "end\n",
    "\n",
    "```\n",
    "\n",
    "**Problem**\n",
    "- Half of the threads are idle on first loop iteration! This is wasteful\n",
    "\n",
    "#### Version 4\n",
    "\n",
    "- First add during load: Let each thread load `OVERLAP` numbers and add them together before switching to shared memory.\n",
    "\n",
    "```julia\n",
    "\n",
    "\"\"\"\n",
    "   reduction_knl_v4(x, y; op=+, ldim::Val{LDIM}=Val(256))\n",
    "\n",
    "Add a few values before switching to shared memory with sequential memory\n",
    "access version where each `LDIM` chunk of values from `x` are reduced using\n",
    "`op` to single values which are stored in the first `gridDim().x` values of `y`\n",
    "\"\"\"\n",
    "function knl_reduction_v4!(y, x, N, op, ::Val{LDIM},\n",
    "                           ::Val{OVERLAP}) where {LDIM, OVERLAP}\n",
    "  tid = threadIdx().x\n",
    "  bid = blockIdx().x\n",
    "  gid = tid + (bid - 1) * LDIM\n",
    "  gsz = LDIM * gridDim().x # total number of threads\n",
    "\n",
    "  l_x = @cuStaticSharedMem(eltype(x), LDIM)\n",
    "\n",
    "  @inbounds begin\n",
    "    # Have each thread initially load and add OVERLAP values\n",
    "    p_x = zero(eltype(x))\n",
    "    for n = 1:OVERLAP\n",
    "      if gid + (n-1) * gsz <= N\n",
    "        p_x += x[gid + (n-1)*gsz]\n",
    "      end\n",
    "    end\n",
    "\n",
    "    l_x[tid] = p_x\n",
    "\n",
    "    sync_threads()\n",
    "\n",
    "    # bit right shift to divide by 2\n",
    "    s = LDIM >> 1\n",
    "    while s > 0\n",
    "      # I'm still active if my thread ID is less than\n",
    "      if tid <= s\n",
    "        # combine my value to my (strided) neighbors value\n",
    "        l_x[tid] = op(l_x[tid], l_x[tid + s])\n",
    "      end\n",
    "      s = s >> 1 # bit right shift to divide by 2 again\n",
    "      sync_threads()\n",
    "    end\n",
    "\n",
    "    # Thread 1 is the only remaining \"real\" thread\n",
    "    tid == 1 && (y[bid] = l_x[tid])\n",
    "  end\n",
    "\n",
    "  nothing\n",
    "end\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
