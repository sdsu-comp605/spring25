{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "072ffef1",
   "metadata": {},
   "source": [
    "# 30) Parallel reductions with CUDA.jl\n",
    "\n",
    "Last time:\n",
    "- Memory management with CUDA.jl\n",
    "- Simple kernels using global memory  \n",
    "- Simple kernels using shared/local memory  \n",
    "- Instruction Level Parallelism  \n",
    "-  Bank Conflicts\n",
    "\n",
    "Today:\n",
    "\n",
    "1. Outline\n",
    "2. Parallel reduction on the GPU\n",
    "3. Different strategies for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b9a2a3",
   "metadata": {},
   "source": [
    "## 1. Outline\n",
    "\n",
    "In this lecture, we will use an efficient parallel reduction on the GPU as an example to talk about:\n",
    "\n",
    "- communication across thread blocks\n",
    "- global synchronization problem across thread blocks\n",
    "- use of different memory pools for optimization strategies\n",
    "\n",
    ":::{note} References:\n",
    "- We will losely follow [Optimizing Parallel Reduction in CUDA](http://developer.download.nvidia.com/assets/cuda/files/reduction.pdf) by Mark Harris. Note that some of our kernels will be different than those given in the talk, but the ideas are (mostly) the same.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d0b9f7",
   "metadata": {},
   "source": [
    "## 2. Parallel reduction on the GPU\n",
    "\n",
    "We want to perform a Parallel Reduction operation. \n",
    "\n",
    "This is a common and important data parallel primitive. It should be:\n",
    "\n",
    "- Easy to implement in CUDA\n",
    "- But often harder to get it right\n",
    "- We can use this as a great optimization example\n",
    "\n",
    "### Basic idea\n",
    "\n",
    "- Tree-based approach used within each thread block\n",
    "\n",
    "![A tree-based approach for a parallel reduction](../img/tree_based_parallel_reduction.png \"A tree-based approach for a parallel reduction\")\n",
    "\n",
    "- Need to be able to use multiple thread blocks\n",
    "  * to process very large arrays\n",
    "  * to keep all multiprocessors on the GPU busy\n",
    "  * each thread block reduces a portion of the array\n",
    "- But how do we communicate partial results between\n",
    "thread blocks?\n",
    "\n",
    "### Problem: Global Synchronization\n",
    "\n",
    "- If we could synchronize across all thread blocks, we _could_ easily reduce very large arrays:\n",
    "  * global sync after each block would produce its result\n",
    "  * once all blocks reach sync, we would continue recursively\n",
    "- But **CUDA has no global synchronization.** Why?\n",
    "  * It's expensive to build in hardware for GPUs with high processor count\n",
    "  * It would force programmers to run fewer blocks (no more than # multiprocessors $\\times$ # resident blocks / multiprocessor) to avoid deadlock, which may reduce overall efficiency \n",
    "\n",
    "**Solution**:  decompose into multiple kernels.\n",
    "\n",
    "- Kernel launch serves as a global synchronization point\n",
    "- Kernel launch has negligible hardware overhead and low software overhead\n",
    "\n",
    "### Solution: Kernel Decomposition\n",
    "\n",
    "- We avoid global sync by decomposing the computation into multiple kernel invocations\n",
    "\n",
    "\n",
    "![A kernel decomposition for a parallel reduction on the GPU](../img/kernel_decomposition_for_reduction.png \"A kernel decomposition for a parallel reduction on the GPU\")\n",
    "\n",
    "- In the case of reductions, the code for all levels is the\n",
    "same: \n",
    "  * we have _recursive_ kernel invocations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf9ed3b",
   "metadata": {},
   "source": [
    "### Measuring success: what are we striving for?\n",
    "\n",
    "- We should strive to reach GPU peak performance\n",
    "\n",
    "- But we need to choose the right metric to measure success:\n",
    "  * GFLOP/s: for compute-bound kernels\n",
    "  * Bandwidth: for memory-bound kernels\n",
    "\n",
    "- Reductions have very low arithmetic intensity\n",
    "  * 1 flop per element loaded (bandwidth-optimal)\n",
    "- Therefore we are _not_ compute-bound, hence we should strive for peak bandwidth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8530ee",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
