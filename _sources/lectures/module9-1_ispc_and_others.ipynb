{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e06d2705",
   "metadata": {},
   "source": [
    "# 31) ISPC, OpenMP target, OpenACC, and all that\n",
    "\n",
    "Last time:\n",
    "\n",
    "- Parallel reductions with CUDA.jl\n",
    "- Different strategies of optmization on the GPU\n",
    "\n",
    "Today: \n",
    "\n",
    "1. ISPC  \n",
    "2. OpenMP target offload  \n",
    "  2.1 Terminology\n",
    "3. OpenACC  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0d5ee1",
   "metadata": {},
   "source": [
    "| Architecture | Directives | SIMD | SPMD |\n",
    "|---------|-----------|------|-----|\n",
    "| Intel AVX+ (SIMD) | `#pragma omp simd` | [intrinsics](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#) | [ISPC](https://ispc.github.io/ispc.html) |\n",
    "| CUDA (SIMT) | `#pragma omp target` | C++ templates and other high-level APIs | CUDA |\n",
    "\n",
    "## 1. [ISPC: Intel SPMD Program Compiler](https://ispc.github.io/ispc.html)\n",
    "\n",
    "The Intel **Implicit SPMD Program Compiler (ISPC)** is a compiler for writing **single program multiple data (SPMD)** programs to run on the CPU and GPU. \n",
    "\n",
    "The SPMD programming approach is similar to approaches used in computer graphics and general-purpose-GPU programming; it is used for GPU shaders and CUDA and OpenCL kernels, for example.\n",
    "\n",
    "- The main idea behind SPMD is that one writes programs as if they were operating on a single data element (a pixel for a pixel shader, for example), but then the underlying hardware and runtime system executes multiple invocations of the program in parallel with different inputs (the values for different pixels, for example).\n",
    "\n",
    "- In summary, we can program **SIMT** (e.g., CUDA) devices using directives, but we can also program **SIMD** (e.g., Intel CPUs) using a **SPMD** (recall, the CUDA-like, acronym that comes from \"single program\" versus \"single instruction\") programming model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5c9a02",
   "metadata": {},
   "source": [
    "```{literalinclude} ../c_codes/module9-1/simple-ispc.ispc\n",
    ":language: c\n",
    ":linenos: true\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bd7bf6",
   "metadata": {},
   "source": [
    "This function is callable from native C code. Example:\n",
    "\n",
    "```{literalinclude} ../c_codes/module9-1/simple.c\n",
    ":language: c\n",
    ":linenos: true\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bb9602a-2b9d-4245-ae23-6abce2140c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcc -O3 -march=native -o simple.o -c ../c_codes/module9-1/simple.c && ispc -O3 --target=avx2-i32x8 ../c_codes/module9-1/simple-ispc.ispc -o simple-ispc.o && gcc simple.o simple-ispc.o  -lm -o simple  && ./simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0748b6f-3212-414d-aae9-414449ac04fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! objdump -d --prefix-addresses -M intel simple | grep sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f07277b",
   "metadata": {},
   "source": [
    "- ISPC is a good option for code with cross-lane dependencies or vector lane divergence (branches that affect some lanes differently than others). \n",
    "\n",
    "- Writing such code with intrinsics is laborious and compilers often do a poor job of inferring good vectorization strategies (despite `#pragma omp simd` and the like). \n",
    "\n",
    "- An example of successful use of ISPC is Intel's [Embree](https://www.embree.org/) ray tracing engine.\n",
    "\n",
    "(As with most vendor-reported performance numbers, we can probably take this with a grain of salt. But it indicates that CPUs remain highly competitive for ray tracing.)\n",
    "\n",
    "![Intel Embree performance](../img/embree-performance.png \"Intel Embree perf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638eb187",
   "metadata": {},
   "source": [
    "## 2. OpenMP target offload\n",
    "\n",
    "- CUDA is relatively hard to maintain and logic/tuning is spread out (between the kernel launch and the device code). \n",
    "- OpenMP target offload and OpenACC attempt to provide a more friendly story for maintenance and incremental migration of legacy code.\n",
    "\n",
    "### Terminology\n",
    "| CUDA Concept | CUDA keyword | OpenACC | OpenMP `target` |\n",
    "|----|------|---------|--------|\n",
    "| Thread block | `blockIdx` | `gang` | `teams` |\n",
    "| Warp | (implicit) | `worker` | thread |\n",
    "| Thread | `threadIdx` | `vector` | `simd` |\n",
    "\n",
    "## 3. OpenACC \n",
    "\n",
    "## Incremental porting with unified memory: OpenACC steps\n",
    "\n",
    "![OpenACC steps](../img/openacc-steps.png \"OpenACC steps\")\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "OpenACC example from a [Lattice-Boltzmann](https://en.wikipedia.org/wiki/Lattice_Boltzmann_methods) miniapp\n",
    "\n",
    "```cpp\n",
    "void LBM::stream(Real* const __restrict a_f,\n",
    "                 const Real* const __restrict a_f_post,\n",
    "                 const int* a_loStr,\n",
    "                 const int* a_hiStr,\n",
    "                 const int* a_loAll,\n",
    "                 const int* a_hiAll,\n",
    "                 const int a_numPts) const\n",
    "{\n",
    "\n",
    "  const int* const __restrict latI = &m_lattice[0][0];\n",
    "  const int* const __restrict latJ = &m_lattice[1][0];\n",
    "  const int* const __restrict latK = &m_lattice[2][0];\n",
    "\n",
    "  const int\n",
    "    klo = a_loStr[2], khi = a_hiStr[2],\n",
    "    jlo = a_loStr[1], jhi = a_hiStr[1],\n",
    "    ilo = a_loStr[0], ihi = a_hiStr[0];\n",
    "\n",
    "#pragma acc parallel loop independent collapse(3) \\\n",
    "        copyin(a_loAll[SPACEDIM],a_hiAll[SPACEDIM],a_f_post[a_numPts*m_numVels]) \\\n",
    "        copyout(a_f[a_numPts*m_numVels]) vector_length(256)\n",
    "  for (int k = klo; k <= khi; ++k) {\n",
    "    for (int j = jlo; j <= jhi; ++j) {\n",
    "      for (int i = ilo; i <= ihi; ++i) {\n",
    "#pragma acc loop seq independent\n",
    "        for (int m = 0; m < NUMV; ++m) {\n",
    "          const long int offset = m * a_numPts;\n",
    "          const long int index0 = INDEX(i          ,           j,           k, a_loAll, a_hiAll);\n",
    "          const long int index2 = INDEX(i - latI[m], j - latJ[m], k - latK[m], a_loAll, a_hiAll);\n",
    "          a_f[index0 + offset]    = a_f_post[index2 + offset];  // new f comes from upwind\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577abeaf",
   "metadata": {},
   "source": [
    "\n",
    "### Resources\n",
    "* [Getting started with OpenACC](https://devblogs.nvidia.com/getting-started-openacc/)\n",
    "* [Advanced OpenACC form a UTK guest lecture](https://icl.utk.edu/~bosilca/classes/cosc462/2016/pdf/OpenACC_Fundamentals.pdf)\n",
    "* [SC18 OpenMP Presentations (with videos)](https://www.openmp.org/resources/openmp-presentations/resources-openmp-presentations-sc18-booth-talks/)\n",
    "* [OpenMP 6.0 Progress and Directions](https://www.openmp.org/wp-content/uploads/OpenMP-api-status-2022.pdf)\n",
    "* [**OpenACC Hackathon series**](https://www.openacc.org/hackathons)\n",
    "* [**Bootcamps**](https://www.openhackathons.org/s/upcoming-events?eventType=bootcamps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
