{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23) Collective communication\n",
    "\n",
    "Last time:\n",
    "- Blocking and non-blocking point-to-point communications\n",
    "\n",
    "Today:\n",
    "1. MPI Collective Communication  \n",
    "2. Minimum Spanning Trees  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MPI Collective Communication\n",
    "\n",
    ":::{tip}\n",
    "Resources for the lecture:\n",
    "1. Article: Chan et al., [_Collective communication: theory, practice, and experience_](https://csu-sdsu.primo.exlibrisgroup.com/permalink/01CALS_SDL/10r4g1c/cdi_crossref_primary_10_1002_cpe_1206)\n",
    "2. Lecture Notes from the University of Texas at Austin: Robert van de Geijn (RVDG) [_Collective Communication: Theory and Practice_](https://www.cs.utexas.edu/~rvdg/tmp/CollectiveCommunication.pdf)\n",
    ":::\n",
    "\n",
    "In the previous lecture we considered point-to-point communication, that is communication between two MPI ranks. In this lecture we will consider communication that involves _all_ the ranks in a communicator.\n",
    "\n",
    "From Fig. 1 in Chan et al.\n",
    "\n",
    "![Figure 1 of Chang et al, Collective communication: theory, practice, and experience article](../img/MPI_collective_comm_Chang_et_al_fig1.png)\n",
    "\n",
    "Types of possible collective communication:\n",
    "\n",
    "- Broadcast: one rank sends data to all the other ranks (RVDG: 103, `MPI_Bcast`)\n",
    "- Reduce(-to-one): Combine (e.g., `sum`, `max`/`min`, etc.) information from all ranks to one rank (RVDG: 104, `MPI_Reduce`)\n",
    "- Scatter: One rank send data to all other ranks (RVDG: 106, `MPI_Scatter`)\n",
    "- Gather: All ranks send data to one rank (RVDG: 107, `MPI_Gather`)\n",
    "- Allgather: All ranks sends information to All ranks (RVDG: 109, `MPI_Allgather`)\n",
    "- Allreduce: All ranks combine information from all ranks (RVDG: 112, `MPI_Allreduce`)\n",
    "- Reduce-scatter: Reduce the send out reduced results (RVDG: 110, `MPI_Reduce_scatter`)\n",
    "\n",
    "Note that there are pairs of reciproval operations:\n",
    "\n",
    "- Broadcast/Reduce(-to-one) (RVDG: 105)\n",
    "- Scatter/Gather (RVDG: 108)\n",
    "- Allgather/Reduce-scatter (RVDG: 111)\n",
    "\n",
    "Two broad classes of collective operations (Chan et al., 1752)\n",
    "\n",
    "- _Data redistribution operations:_ Broadcast, scatter, gather, and allgather. These operations move data between processors.\n",
    "- _Data consolidation operations:_ Reduce(-to-one), reduceâ€“scatter, and allreduce. These operations consolidate contributions from different processors by applying a reduction operation. We will only consider reduction operations that are both commutative and associative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Minimum Spanning Trees\n",
    "\n",
    "### 2.1 Broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
