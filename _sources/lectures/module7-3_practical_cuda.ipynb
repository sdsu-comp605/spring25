{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f134ec",
   "metadata": {},
   "source": [
    "# 27) Practical CUDA\n",
    "\n",
    "Last Time:\n",
    "\n",
    "- GPUs and CUDA\n",
    "- Kernel syntax examples\n",
    "- Thread hirerachy\n",
    "- Memory\n",
    "\n",
    "Today:\n",
    "\n",
    "1. When to use a GPU?  \n",
    "2. Practical CUDA  \n",
    "3. Memory  \n",
    "  3.1 On memory coalescing and strided access  \n",
    "4. Tuckoo demo for CUDA codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfcf382",
   "metadata": {},
   "source": [
    "## 1. When to use a GPU?\n",
    "\n",
    "* GPUs have 2-4x greater floating point and bandwidth peak for the watts\n",
    "  * also for the \\$ if you buy enterprise gear\n",
    "  * better for the \\$ if you buy gaming gear\n",
    "* Step 1 is to assess workload and latency requirements\n",
    "\n",
    "![VecDot CPU vs GPU size on the x-axis](../img/VecDot_CPU_vs_GPU_size.png \"VecDot CPU vs GPU size on the x-axis\")\n",
    "![VecDot CPU vs GPU size on the x-axis](../img/VecDot_CPU_vs_GPU_time.png \"VecDot CPU vs GPU time on the x-axis\")\n",
    "\n",
    "* Don't waste time with GPUs if\n",
    "  * your problem size or time to solution requirements don't align\n",
    "  * if the work you'd like to move to the GPU is not a bottleneck\n",
    "  * if the computation cost will be dwarfed by moving data to/from the GPU\n",
    "    * often you need to restructure so that caller passes in data already on the device\n",
    "    * can require nonlocal refactoring\n",
    "* Almost never: pick one kernel at a time and move it to the GPU\n",
    "  * Real-world examples: DOE ACME/E3SM projects (to pick on one high-profile application) has basically done this for five years and it still doesn't help their production workloads so they bought a non-GPU machine\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1419a410",
   "metadata": {},
   "source": [
    "### Okay, okay, okay.  What if I have the right workload?\n",
    "\n",
    "#### Terminology/Intro\n",
    "\n",
    "* [An even easier introduction to CUDA](https://devblogs.nvidia.com/even-easier-introduction-cuda/)\n",
    "* [CUDA Programming Model](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model)\n",
    "\n",
    "* On the CPU, we have a thread with vector registers/instructions\n",
    "* In CUDA, we write code inside a single vector lane (\"confusingly\" called a CUDA thread)\n",
    "* To get inside the lane, we launch a **kernel** from the CPU using special syntax. For example:\n",
    "\n",
    "```c\n",
    "add<<<numBlocks, blockSize>>>(N, x, y);\n",
    "```\n",
    "\n",
    "* needs to be compiled using `nvcc` compiler\n",
    "* Logically 1D/2D/3D rectangular tiled iteration space\n",
    "\n",
    "![CUDA: grid of thread blocks](../img/grid-of-thread-blocks.png \"CUDA: grid of thread blocks\")\n",
    "\n",
    "\n",
    "* There are [many](https://en.wikipedia.org/wiki/CUDA#Version_features_and_specifications) constraints and limitations to the iteration \"grid\"\n",
    "\n",
    "![CUDA constraints](../img/cuda-constraints.png \"CUDA constraints\")\n",
    "\n",
    "* Control flow for CUDA threads is nominally independent, but performance will be poor if you don't coordinate threads within each block.\n",
    "  * Implicit coordination:\n",
    "    * Memory coalescing\n",
    "    * Organize your algorithm to limit \"divergence\"\n",
    "  * Explicit coordination:\n",
    "    * Shared memory\n",
    "    * `__syncthreads()`\n",
    "    * Warp shuffles\n",
    "* We implement the kernel by using the `__global__` attribute\n",
    "  * Visible from the CPU\n",
    "  * Special [built-in variables](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#built-in-variables) are defined\n",
    "    * `gridDim`: dimension of the grid\n",
    "    * `blockIdx`: block index within the grid\n",
    "    * `blockDim`: dimensions of the block\n",
    "    * `threadIdx`: thread index within the block.\n",
    "  * There is also `__device__`, which is callable from other device functions\n",
    "  * Can use `__host__ __device__` to compile two versions\n",
    "\n",
    "![CUDA indexing](../img/cuda_indexing.png \"CUDA indexing\")\n",
    "\n",
    "#### How does this relate to the hardware?\n",
    "\n",
    "* Each thread block is assigned to one **streaming multiprocessor (SM)**\n",
    "* Executed in warps (number of hardware lanes)\n",
    "* Multiple warps (from the same or different thread blocks) execute like \"hyperthreads\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e7efd",
   "metadata": {},
   "source": [
    "## 2. Practical CUDA  \n",
    "\n",
    "### [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html)\n",
    "\n",
    "#### Occupancy\n",
    "\n",
    "> Thread instructions are executed sequentially in CUDA, and, as a result, executing other warps when one warp is paused or stalled is the only way to hide latencies and keep the hardware busy. Some metric related to the number of active warps on a multiprocessor is therefore important in **determining how effectively the hardware is kept busy**. This metric is _occupancy_.  [emphasis added]\n",
    "\n",
    "* Reality: occupancy is just one aspect, and often inversely correlated with keeping the hardware busy (and with performance).\n",
    "\n",
    "> Occupancy is the ratio of the number of active warps per multiprocessor to the maximum number of possible active warps.\n",
    "\n",
    "* If your kernel uses fewer registers/less shared memory, more warps can be scheduled.\n",
    "* Register/shared memory usage is determined by the compiler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f2da48",
   "metadata": {},
   "source": [
    "Code example: \n",
    "\n",
    "```{literalinclude} ../cuda_codes/module7-3/add.cu\n",
    ":language: cuda\n",
    ":linenos: true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8fdb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvcc -c ../cuda_codes/module7-3/add.cu --resource-usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02531e8",
   "metadata": {},
   "source": [
    "- This shows us [PTX](https://docs.nvidia.com/cuda/parallel-thread-execution/) information, where **PTX** is a low-level parallel thread execution virtual machine and instruction set architecture (ISA). PTX exposes the GPU as a data-parallel computing device. \n",
    "- [`ptxas`](https://docs.nvidia.com/cuda/parallel-thread-execution/#syntax): PTX programs are a collection of text source modules (files). PTX source modules have an assembly-language style syntax with instruction operation codes and operands. Pseudo-operations specify symbol and addressing management. The `ptxas` optimizing backend compiler optimizes and assembles PTX source modules to produce corresponding binary object files.\n",
    "\n",
    "**Understanding the `ptxas info`**:\n",
    "- Find the documentation page [here](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#printing-code-generation-statistics).\n",
    "  * `gmem`: global memory\n",
    "  * `stack frame` is per thread stack usage used by this function. \n",
    "  * `spill stores` and `spill loads` represent stores and loads done on stack memory which are being used for storing variables that couldnâ€™t be allocated to physical registers.\n",
    "  * Similarly number of `registers`, amount of total space in constant bank, `cmem`, allocated is shown.\n",
    "  * On more modern versions, also amount of shared memory, `smem`, can be shown.\n",
    "\n",
    "Remarks:\n",
    "- A stack is not a concept that is unique or specific to CUDA. A stack frame is simply the space utilized by the stack, or the space utilized to conduct a particular operation on/with the stack (such as a function or subroutine call).\n",
    "- Spill stores and spill loads relate to the usage of variables in the logical local space. Such variables may manifest in a register, or they may manifest in DRAM memory, or perhaps the caches. \n",
    "  * The GPU is a load/store architecture for the most part, so when it comes time to use variables in calculations, they almost universally manifest in GPU registers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3755df",
   "metadata": {},
   "source": [
    "```{literalinclude} ../cuda_codes/module7-3/copy.cu\n",
    ":language: cuda\n",
    ":linenos: true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39155dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvcc -c ../cuda_codes/module7-3/copy.cu --resource-usage -DTILE_SIZE=16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a590a8",
   "metadata": {},
   "source": [
    "* The [NVIDIA Nsight Occupancy Calculator](https://docs.nvidia.com/nsight-compute/NsightCompute/index.html#occupancy-calculator) can compute occupancy based on the register and shared memory usage.\n",
    "* You can tell the compiler to reduce register usage, sometimes at the expense of spills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb5508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvcc -c ../cuda_codes/module7-3/copy.cu --resource-usage -DTILE_SIZE=16 --maxrregcount 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3223b79",
   "metadata": {},
   "source": [
    "In the example above, we have used the [`--maxrregcount`](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html?highlight=maxrregcount#maxrregcount-amount-maxrregcount) compiler flag that specifies the maximum amount of registers that GPU functions can use.\n",
    "  >  Until a function-specific limit, a higher value will generally increase the performance of individual GPU threads that execute this function. However, because thread registers are allocated from a global register pool on each GPU, a higher value of this option will also reduce the maximum thread block size, thereby reducing the amount of thread parallelism. Hence, a good `maxrregcount` value is the result of a _trade-off_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36d3017",
   "metadata": {},
   "source": [
    "#### Further reading:\n",
    "\n",
    "* Vasily Volkov (2010) [**Better Performance at Lower Occupancy**](https://www.nvidia.com/content/GTC-2010/pdfs/2238_GTC2010.pdf) (slides)\n",
    "* Vasily Volkov (2016) [**Understanding Latency Hiding on GPUs**](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-143.pdf) (very in-depth)\n",
    "* Kasia Swirydowicz (2018) [**Finite Element Stiffness Matrix Action: monolithic kernel optimization on Titan V**](https://www.paranumal.com/single-post/2018/03/02/Finite-Element-Stiffness-Matrix-Action-monolithic-kernel-optimization-on-Titan-V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f7f31",
   "metadata": {},
   "source": [
    "## 3. Memory\n",
    "\n",
    "\n",
    "* GPU memory is _not_ CPU memory\n",
    "\n",
    "![A single socket of the Summit supercomputer](https://en.wikichip.org/w/images/4/47/summit_single-socket.svg)\n",
    "\n",
    "**Duh**, so why does NVIDIA [publish this](https://devblogs.nvidia.com/unified-memory-cuda-beginners/)?\n",
    "\n",
    "![Image from nvidia.com Unified Memory for CUDA Beginners page](https://devblogs.nvidia.com/wp-content/uploads/2017/06/Unified-Memory-MultiGPU.png)\n",
    "\n",
    "**Getting your memory into position is often the hardest part of CUDA programming.**\n",
    "\n",
    "You need to:\n",
    "\n",
    "* Allocate memory on the GPU:\n",
    "\n",
    "```c\n",
    "cudaMalloc(&xdevice, N*sizeof(double));\n",
    "```\n",
    "\n",
    "* Populate it from the host:\n",
    "\n",
    "```c\n",
    "cudaMemcpy(xdevice, xhost, N*sizeof(double), cudaMemcpyHostToDevice);\n",
    "```\n",
    "\n",
    "* Repeat for all data, including control parameters\n",
    "* Easy to forget, ongoing maintenance/complexity cost\n",
    "\n",
    "### [Unified/managed memory](https://devblogs.nvidia.com/unified-memory-cuda-beginners/)\n",
    "\n",
    "> This hardware/software technology allows applications to allocate data that can be read or written from code running on either CPUs or GPUs. Allocating Unified Memory is as simple as replacing calls to `malloc()` or `new` with calls to `cudaMallocManaged()`, an allocation function that returns a pointer accessible from any processor. \n",
    "\n",
    "* Allocate \"managed\" memory, accessible from CPU and GPU:\n",
    "\n",
    "```c\n",
    "cudaMallocManaged(&x, N*sizeof(float));\n",
    "```\n",
    "\n",
    "* How?\n",
    "\n",
    "![Maximizing Unified Memory](../img/maximizing-unified-memory.png \"Maximizing Unified Memory\")\n",
    "\n",
    "* With OpenACC, you can make all dynamic allocations in managed memory. Example: `pgcc -ta=tesla:managed`\n",
    "  * The GPU probably has less memory than you have DRAM\n",
    "  * Really convenient for incremental work in legacy code\n",
    "  * Performance isn't great without `cudaMemPrefetchAsync`\n",
    "  \n",
    "![Streaming performance unified memory GPU](../img/streaming_performance_unified_memory_gpu.png \"Streaming performance unified memory GPU\")\n",
    "\n",
    "\n",
    "**Further reading**: [Maximizing Unified Memory Performance in CUDA](https://devblogs.nvidia.com/maximizing-unified-memory-performance-cuda/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d27de3",
   "metadata": {},
   "source": [
    "### 3.1 On memory coalescing and [strided access](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#strided-accesses)\n",
    "\n",
    "\n",
    "```c\n",
    "__global__ void strideCopy(float *odata, float* idata, int stride) {\n",
    "    int xid = (blockIdx.x*blockDim.x + threadIdx.x)*stride;\n",
    "    odata[xid] = idata[xid];\n",
    "}\n",
    "```\n",
    "\n",
    "This kernel copies data with a stride of `stride` elements between threads from `idata` to `odata`.\n",
    "\n",
    "> ensuring that as much as possible of the data in each cache line fetched is actually used is an important part of performance optimization of memory accesses on these devices.\n",
    "\n",
    "![](../img/adjacent-threads-accessing-memory-with-stride-of-2.png)\n",
    "\n",
    "In this case, threads within a warp access words in memory with a stride of 2. This action leads to a load of eight L2 cache segments per warp on a Tesla V100 (compute capability 7.0).\n",
    "\n",
    "We lose half of the bandwidth for `stride=2`:\n",
    "\n",
    "\n",
    "In fact, a stride of 2 results in a 50% of load/store efficiency since half the elements in the transaction are not used and represent wasted bandwidth. \n",
    "\n",
    "As the stride increases, the effective bandwidth decreases until the point where 32 32-byte segments are loaded for the 32 threads in a warp.\n",
    "\n",
    "We can do better by aligning the memory loads, stride size and warp size.\n",
    "\n",
    "\n",
    "![nvidia.com website performance of stridecopy kernel](../img/performance-of-stridecopy-kernel.png \"nvidia.com website performance of stridecopy kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1484301f",
   "metadata": {},
   "source": [
    "## 4. Tuckoo demo for CUDA codes\n",
    "\n",
    "The Tuckoo cluster is equipped with four NVIDIA P100s. To be able to compile and execute CUDA code on one of the GPU nodes, we first need to log in on an _interactive node_. \n",
    "\n",
    "An _interactive node_ on a cluster is often a node that is dedicated to quick compilation/testing/debugging of your codes-**not** to high-demanding or large-scale computations-. If you misuse an interactive node, a cluster admin might kick you out of that node.\n",
    "\n",
    "- For tuckoo, **`node8`** is the node dedicated for interactive usage, where you can find the `nvcc` compiler to compile CUDA codes.\n",
    "\n",
    "Once you are logged in on tuckoo using \n",
    "\n",
    "```shell\n",
    "ssh your_user_name@tuckoo.sdsu.edu\n",
    "```\n",
    "\n",
    "You will see that you are logged in on the _login node_ because the prompt will show\n",
    "\n",
    "```shell\n",
    "[your_user_name@tuckoo ~]$\n",
    "```\n",
    "\n",
    "Now you can use `rsh` to connect to `node8` by simply running:\n",
    "\n",
    "```shell\n",
    "rsh node8\n",
    "```\n",
    "\n",
    "and you will see that you are indeed on `node8` because the prompt has changed to:\n",
    "\n",
    "```shell\n",
    "[your_user_name@node8 ~]$\n",
    "```\n",
    "\n",
    "Now you can compile any CUDA code by invoking the `nvcc` compiler and execute it by sending it to the Slurm batch scheduler via a batch script. \n",
    "\n",
    "You can find an example of a batch script to execute our `./hello_world` cuda code in [`batch_scripts/batch-cuda`](https://github.com/sdsu-comp605/spring25/tree/main/batch_scripts/batch-cuda).\n",
    "\n",
    "```{literalinclude} ../batch_scripts/batch-cuda\n",
    ":language: shell\n",
    ":linenos: true\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
