{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "072ffef1",
   "metadata": {},
   "source": [
    "# 30) Parallel reductions with CUDA.jl\n",
    "\n",
    "Last time:\n",
    "- Memory management with CUDA.jl\n",
    "- Simple kernels using global memory  \n",
    "- Simple kernels using shared/local memory  \n",
    "- Instruction Level Parallelism  \n",
    "-  Bank Conflicts\n",
    "\n",
    "Today:\n",
    "\n",
    "1. Outline\n",
    "2. Parallel reduction on the GPU\n",
    "3. Parallel reduction complexity\n",
    "4. Different strategies for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b9a2a3",
   "metadata": {},
   "source": [
    "## 1. Outline\n",
    "\n",
    "In this lecture, we will use an efficient parallel reduction on the GPU as an example to talk about:\n",
    "\n",
    "- communication across thread blocks\n",
    "- global synchronization problem across thread blocks\n",
    "- use of different memory pools for optimization strategies\n",
    "\n",
    ":::{note} References:\n",
    "- We will losely follow [Optimizing Parallel Reduction in CUDA](http://developer.download.nvidia.com/assets/cuda/files/reduction.pdf) by Mark Harris. Note that some of our kernels will be different than those given in the talk, but the ideas are (mostly) the same.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d0b9f7",
   "metadata": {},
   "source": [
    "## 2. Parallel reduction on the GPU\n",
    "\n",
    "We want to perform a Parallel Reduction operation on an array of size $N$. \n",
    "\n",
    "This is a common and important data parallel primitive. It should be:\n",
    "\n",
    "- Easy to implement in CUDA\n",
    "- But often harder to get it right\n",
    "- We can use this as a great optimization example\n",
    "\n",
    "### Basic idea\n",
    "\n",
    "- Tree-based approach used within each thread block\n",
    "\n",
    "![A tree-based approach for a parallel reduction](../img/tree_based_parallel_reduction.png \"A tree-based approach for a parallel reduction\")\n",
    "\n",
    "- Need to be able to use multiple thread blocks\n",
    "  * to process very large arrays\n",
    "  * to keep all multiprocessors on the GPU busy\n",
    "  * each thread block reduces a portion of the array\n",
    "- But how do we communicate partial results between\n",
    "thread blocks?\n",
    "\n",
    "### Problem: Global Synchronization\n",
    "\n",
    "- If we could synchronize across all thread blocks, we _could_ easily reduce very large arrays:\n",
    "  * global sync after each block would produce its result\n",
    "  * once all blocks reach sync, we would continue recursively\n",
    "- But **CUDA has no global synchronization.** Why?\n",
    "  * It's expensive to build in hardware for GPUs with high processor count\n",
    "  * It would force programmers to run fewer blocks (no more than # multiprocessors $\\times$ # resident blocks / multiprocessor) to avoid deadlock, which may reduce overall efficiency \n",
    "\n",
    "**Solution**:  decompose into multiple kernels.\n",
    "\n",
    "- Kernel launch serves as a global synchronization point\n",
    "- Kernel launch has negligible hardware overhead and low software overhead\n",
    "\n",
    "### Solution: Kernel Decomposition\n",
    "\n",
    "- We avoid global sync by decomposing the computation into multiple kernel invocations\n",
    "\n",
    "\n",
    "![A kernel decomposition for a parallel reduction on the GPU](../img/kernel_decomposition_for_reduction.png \"A kernel decomposition for a parallel reduction on the GPU\")\n",
    "\n",
    "- In the case of reductions, the code for all levels is the\n",
    "same: \n",
    "  * we have _recursive_ kernel invocations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf9ed3b",
   "metadata": {},
   "source": [
    "### Measuring success: what are we striving for?\n",
    "\n",
    "- We should strive to reach GPU peak performance\n",
    "\n",
    "- But we need to choose the right metric to measure success:\n",
    "  * GFLOP/s: for compute-bound kernels\n",
    "  * Bandwidth: for memory-bound kernels\n",
    "\n",
    "- Reductions have very low arithmetic intensity\n",
    "  * 1 flop per element loaded (bandwidth-optimal)\n",
    "- Therefore we are _not_ compute-bound, hence we should strive for peak bandwidth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb4e187",
   "metadata": {},
   "source": [
    "## 3. Parallel reduction complexity\n",
    "\n",
    "- $\\log(N)$ parallel steps, each step $S$ does $N/2^S$ independent operations\n",
    "    * _Step complexity_ is $O(\\log N)$\n",
    "- For $N=2^D$ (we are sticking to power-of-2 block sizes), performs $\\Sigma_{S \\in [1, \\dots, D]} 2^{D-S} = N-1$ operations\n",
    "    * _Work complexity_ is $O(N)$ - It is _work-efficient_, i.e., does not perform more operations than a sequential algorithm \n",
    "- With $P$ threads physically in parallel ($P$ processors), _time complexity_ is $O(\\frac{N}{P} + \\log N)$\n",
    "    * Compare to $O(N)$ for sequential reduction\n",
    "    * In a thread block, $N=P$, so $O(\\log N)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7389e1",
   "metadata": {},
   "source": [
    "## 4. Different strategies for optimization\n",
    "\n",
    "### Basic implementation idea:\n",
    "\n",
    "#### Version 1\n",
    "\n",
    "Because there is no global synchronization, we need to launch the kernel multiple times in _work groups_. \n",
    "\n",
    "In each launch, each work group will work together to reduce the set of numbers to a single summed value.\n",
    "\n",
    "- In the first iterations we launch, say, `num_groups` work groups to reduce a vector of length `N` to a vector of length `num_groups`. \n",
    "\n",
    "- We will then recurse and reduce the vector of length `num_groups` to a smaller vector, and continue until we only have `1` element in the partially reduced vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6e7d9a",
   "metadata": {},
   "source": [
    "#### Version 2\n",
    "\n",
    "- Work-group size elements of the given array into shared memory\n",
    "- Use binary sum to reduce the values to a single number\n",
    "- Recurse until there is only one group\n",
    "\n",
    "#### Version 3\n",
    "\n",
    "- Use sequential addressing of main memory\n",
    "\n",
    "![Sequential addressing reduction](../img/sequential_addressing_reduction.png \"Sequential addressing reduction\")\n",
    "\n",
    "- This version with sequential addressing is conflict free (i.e., no bank conflicts)\n",
    "\n",
    "#### Version 4\n",
    "\n",
    "- Let each thread load `OVERLAP` numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8530ee",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
