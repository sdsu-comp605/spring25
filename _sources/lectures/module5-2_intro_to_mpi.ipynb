{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18) Introduction to MPI\n",
    "\n",
    "Last time:\n",
    "- Parallel reductions and scans\n",
    "- Graphs\n",
    "\n",
    "Today:\n",
    "\n",
    "1. [Processes and Threads](#processes-and-threads)  \n",
    "2. [MPI: Message Passing Interface](#mpi-message-passing-interface)  \n",
    "  2.1 [Communicators](#communicators)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Processes and Threads\n",
    "\n",
    "Threads and processes are indeed very similar.\n",
    "\n",
    "Similarities:\n",
    "\n",
    "* Both created via [`clone` system call](https://linux.die.net/man/2/clone) on Linux.\n",
    "* `clone` allows the child process to share parts of its execution context with the calling process, such as the memory space, the table of file descriptors, and the table of signal handlers. \n",
    "* The main use of `clone()` is to implement threads: multiple threads of control in a program that run concurrently in a **shared memory** space.\n",
    "* Threads and processes are scheduled in the same way by the operating system\n",
    "* They have separate stacks (automatic variables)\n",
    "* They have access to same memory before `fork()` or `clone()`.\n",
    "\n",
    "But some important distinctions:\n",
    "\n",
    "* Threads set `CLONE_VM`: which means the calling process and the child process run in the _same memory space_. In particular, memory writes performed by the calling process or by the child process are also visible in the other process. \n",
    "* Threads _share_ the same virtual-to-physical address mapping.\n",
    "* Threads can access the same data at the same addresses; `private` data is private only because other threads don't know its address.\n",
    "* Threads set `CLONE_FILES` (which means the calling process and the child process share the same file descriptor table).\n",
    "* Threads set `CLONE_THREAD` (hence, the child is placed in the same thread group as the calling process. ) and `CLONE_SIGHAND` (the calling process and the child process share the same table of signal handlers)\n",
    "* Process id and signal handlers are shared\n",
    "\n",
    "### Myths about processes\n",
    "\n",
    "* Processes can't share memory\n",
    "  * Not true. See: `mmap()`, `shm_open()`, and `MPI_Win_allocate_shared()`\n",
    "* Processes are \"heavy\"\n",
    "  * They actually share same data structures and kernel scheduling; no difference in context switching\n",
    "  * Startup costs: ~100 microseconds to duplicate page tables\n",
    "  * Caches are physically tagged; processes can share L1 cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. [MPI: Message Passing Interface](https://en.wikipedia.org/wiki/Message_Passing_Interface)\n",
    "\n",
    "* Just a library: you will find it in your plain C, C++, or Fortran compiler (just like OpenMP)\n",
    "* Two active open source library implementations: [MPICH](https://www.mpich.org/) and [Open MPI](https://www.open-mpi.org/)\n",
    "* Numerous vendor implementations modify/extend these open source implementations\n",
    "* MVAPICH is an MPICH-derived open source implementation for InfiniBand and related networks\n",
    "* Bindings from many other languages; for instance, [mpi4py](https://mpi4py.readthedocs.io/en/stable/) is popular for Python and [MPI.jl](https://github.com/JuliaParallel/MPI.jl) for Julia\n",
    "* Scales to millions of processes across ~100k nodes\n",
    "  * Shared memory systems can be scaled up to [~4000 cores](https://www.uvhpc.com/sgi-uv-3000), but latency and price ($) increase\n",
    "* Standard usage: processes are separate on startup\n",
    "* Timeline\n",
    "  * MPI-1 (1994) point-to-point messaging, collectives\n",
    "  * MPI-2 (1997) parallel IO, dynamic processes, one-sided\n",
    "  * MPI-3 (2012) nonblocking collectives, neighborhood collectives, improved one-sided\n",
    "\n",
    "\n",
    "Let's see our very fist C example that uses MPI API functions. You can find this code in [c_codes/module5-2/mpi-demo.c](https://github.com/sdsu-comp605/spring25/tree/main/c_codes/module5-2/mpi-demo.c).\n",
    "\n",
    "\n",
    "```{literalinclude} ../c_codes/module5-2/mpi-demo.c\n",
    ":language: c\n",
    ":linenos: true\n",
    "```\n",
    "\n",
    "This may remind you of the top-level OpenMP strategy\n",
    "```c\n",
    "int main() {\n",
    "    #pragma omp parallel\n",
    "    {\n",
    "        int rank = omp_get_thread_num();\n",
    "        int size = omp_get_num_threads();\n",
    "        // your code\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We use the compiler wrapper `mpicc`, but it just passes some flags to the real compiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! mpicc -show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! mpicc -Wall    ../c_codes/module5-2/mpi-demo.c   -o mpi-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute: \n",
    "* We use `mpiexec` to run locally.  Clusters/supercomputers often have different job launching programs (such as `srun` or `mpirun`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! mpiexec -n 2 ./mpi-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In MPI terminology, `ranks` is the same as processes\n",
    "\n",
    "* We can run more MPI processes than cores (or hardware threads), but you might need to use the `--oversubscribe` option because **oversubscription is usually expensive**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! mpiexec -n 6 --oversubscribe ./mpi-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can use OpenMP _within_ ranks of MPI (but use `MPI_Init_thread()` in your program)\n",
    "* Everything is _private_ by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Advice from Bill Gropp](   https://www.rce-cast.com/Podcast/rce-28-mpich2.html):\n",
    "\n",
    ">   You want to think about how you decompose your data structures, how\n",
    "    you think about them globally.  [...]  If you were building a house,\n",
    "    you'd start with a set of blueprints that give you a picture of what\n",
    "    the whole house looks like.  You wouldn't start with a bunch of\n",
    "    tiles and say. \"Well I'll put this tile down on the ground, and\n",
    "    then I'll find a tile to go next to it.\"  But all too many people\n",
    "    try to build their parallel programs by creating the smallest\n",
    "    possible tiles and then trying to have the structure of their code\n",
    "    emerge from the chaos of all these little pieces.  You have to have\n",
    "    an organizing principle if you're going to survive making your code\n",
    "    parallel.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Communicators\n",
    "\n",
    "* `MPI_COMM_WORLD` contains all ranks in the `mpiexec`.  Those ranks may be on different nodes, even in different parts of the world.\n",
    "* `MPI_COMM_SELF` contains only one rank\n",
    "* Can create new communicators from existing ones\n",
    "\n",
    "```c\n",
    "int MPI_Comm_dup(MPI_Comm comm, MPI_Comm *newcomm); // To duplicate\n",
    "int MPI_Comm_split(MPI_Comm comm, int color, int key, MPI_Comm *newcomm); // can split based on colors and keys\n",
    "int MPI_Comm_create(MPI_Comm comm, MPI_Group group, MPI_Comm *newcomm); // Creates a new communicator\n",
    "```\n",
    "* Can spawn new processes (but not supported on all machines)\n",
    "```c\n",
    "int MPI_Comm_spawn(const char *command, char *argv[], int maxprocs,\n",
    "            MPI_Info info, int root, MPI_Comm comm,\n",
    "            MPI_Comm *intercomm, int array_of_errcodes[]);\n",
    "```\n",
    "* Can attach _attributes_ to communicators (useful for library composition)\n",
    "\n",
    "### Collective operations\n",
    "\n",
    "MPI has a rich set of collective operations scoped by communicator, including the following.\n",
    "\n",
    "```c\n",
    "int MPI_Reduce(const void *sendbuf, void *recvbuf, int count,\n",
    "        MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm); // Reduces values on all processes to a single value\n",
    "int MPI_Allreduce(const void *sendbuf, void *recvbuf, int count,\n",
    "        MPI_Datatype datatype, MPI_Op op, MPI_Comm comm); // Combines values from all processes and distributes the result back to all processes\n",
    "int MPI_Scan(const void *sendbuf, void *recvbuf, int count,\n",
    "        MPI_Datatype datatype, MPI_Op op, MPI_Comm comm); // is an inclusive scan: it performs a prefix reduction across all MPI processes in the given communicator\n",
    "int MPI_Gather(const void *sendbuf, int sendcount, MPI_Datatype sendtype,\n",
    "        void *recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm); // Gathers together values from a group of processes\n",
    "int MPI_Scatter(const void *sendbuf, int sendcount, MPI_Datatype sendtype,\n",
    "        void *recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm); // Sends data from one process to all other processes in a communicator\n",
    "```\n",
    "\n",
    "* Implementations are optimized by vendors for their custom networks, and can be very fast.\n",
    "\n",
    "Plot from [Paul Fischer](https://www.mcs.anl.gov/~fischer/), researcher at Argonne National Labb and Professor at UIUC:\n",
    "\n",
    "![Fischer BGP plot](https://www.mcs.anl.gov/~fischer/gop/bgp_gop_png.png)\n",
    "\n",
    "Notice how the time is basically independent of the number of processes $P$, and only a small multiple of the cost to send a single message. Not all networks are this good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point-to-point messaging\n",
    "\n",
    "In addition to collectives, MPI supports messaging directly between individual ranks.\n",
    "\n",
    "![MPI send-recv](../img/mpi-send-recv.png \"MPI Send-recv\")\n",
    "\n",
    "In the above sketch, `MPI_Isend` and `MPI_Irecv` are **non-blocking**. The \"I\" stands for \"with Immediate return\"; it does not block until the message is received. In fact:\n",
    "* Interfaces can be:\n",
    "  * blocking like `MPI_Send()` and `MPI_Recv()`, or\n",
    "  * \"immediate\" (asynchronous), like `MPI_Isend()` and `MPI_Irecv()`.  The immediate varliants return an `MPI_Request`, which must be waited on to complete the send or receive.\n",
    "* Be careful of deadlock when using blocking interfaces.\n",
    "  * I never use blocking send/recv.\n",
    "  * There are also \"synchronous\" `MPI_Ssend` and \"buffered\" `MPI_Bsend`, and nonblocking variants of these, `MPI_Issend`, etc.\n",
    "* Point-to-point messaging is like the assembly of parallel computing\n",
    "  * It can be good for building libraries, but it's a headache to use directly for most purposes\n",
    "  * Better to use collectives when possible, or higher level libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighbors\n",
    "\n",
    "A common pattern involves communicating with neighbors, often many times in sequence (such as each iteration or time step).\n",
    "\n",
    "![MPI neighbor communicator](../img/mpi-neighbor-grid.png \"MPI Neighbor grid\")\n",
    "\n",
    "This can be achieved with\n",
    "* Point-to-point communication: `MPI_Isend`, `MPI_Irecv`, `MPI_Waitall`\n",
    "* Persistent: `MPI_Send_init` (once), `MPI_Startall`, `MPI_Waitall`.\n",
    "* Neighborhood collectives (need to create special communicator)\n",
    "* One-sided (need to manage safety yourself)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
