{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83d3d99e",
   "metadata": {},
   "source": [
    "# 28) Intro to GPU programming in Julia \n",
    "\n",
    "Last time:\n",
    "- Practical CUDA\n",
    "- Memory\n",
    "- Tuckoo demo for CUDA codes\n",
    "\n",
    "Today:\n",
    "1. Intro to GPU programming in Julia (CUDA.jl)  \n",
    "2. CUDA Julia code on the tuckoo cluster  \n",
    "   2.1 Launching a CUDA.jl code with SLURM  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3fdc47",
   "metadata": {},
   "source": [
    "## 1. Intro to GPU programming in Julia (CUDA.jl)\n",
    "\n",
    "For GPU programming there are many great resources. Some that I may refer to are:\n",
    "\n",
    ":::{note} References\n",
    "- [Warburton | youtube video](https://www.youtube.com/watch?v=uvVy3CqpVbM) (In the first 47 minutes of the video, Tim gives an excellent introduction to the GPU.)\n",
    "- [Warburton | ATPESC pdf](https://extremecomputingtraining.anl.gov/files/2018/08/ATPESC_2018_Track-2_3_8-2_830am_Warburton-Accelerators.pdf)\n",
    ":::\n",
    "\n",
    "### Add vectors\n",
    "\n",
    "The real \"hello world\" of the GPU is adding vectors:\n",
    "\n",
    "$$\n",
    "C = A + B\n",
    "$$\n",
    "\n",
    "Big ideas:\n",
    "\n",
    "- threads\n",
    "- blocks\n",
    "- how to launch and time kernels\n",
    "- off-loaded memory on the device\n",
    "\n",
    "### Example with [CUDA.jl](https://github.com/JuliaGPU/CUDA.jl)\n",
    "\n",
    "To run this and following Julia CUDA examples, you need to first add [CUDA.jl](https://github.com/JuliaGPU/CUDA.jl) to your environment. That is, start `julia` and in the REPL run:\n",
    "\n",
    "```julia\n",
    "using Pkg\n",
    "Pkg.add(\"CUDA\")\n",
    "```\n",
    "\n",
    "Then you can execute the following script:\n",
    "\n",
    "\n",
    "```{literalinclude} ../julia_codes/module8-1/add_cu_arrays.jl\n",
    ":language: julia\n",
    ":linenos: true\n",
    "```\n",
    "\n",
    "### Memory management\n",
    "\n",
    "As we have seen so far, a crucial aspect of working with a GPU is managing the data on it. \n",
    "\n",
    "The `CuArray` type is the primary interface for doing so: Creating a `CuArray` will allocate data on the GPU, copying elements to it will upload, and converting back to an `Array` will download values to the CPU. Let's see it in an example test. \n",
    "\n",
    "Note: to run the following test, you first need to also add the `Test.jl` package to your environment:\n",
    "\n",
    "\n",
    "```julia\n",
    "using Pkg\n",
    "Pkg.add(\"Test\")\n",
    "```\n",
    "\n",
    "Then you can run the following test example:\n",
    "\n",
    "```{literalinclude} ../julia_codes/module8-1/copy_cu_array.jl\n",
    ":language: julia\n",
    ":linenos: true\n",
    "```\n",
    "\n",
    "**Observation on garbage collection**:\n",
    "\n",
    "- One striking difference between the native C CUDA implementation and the Julia CUDA.jl interface is that instances of the `CuArray` type are managed by the Julia garbage collector. This means that they will be collected once they are unreachable, and the memory hold by it will be repurposed or freed. There is _no need_ for manual memory management (like the `cudaFree`), just make sure your objects are not reachable (i.e., there are no instances or references).\n",
    "\n",
    "### Reverse vectors\n",
    "\n",
    "The \"hello world 2.0\" of the GPU is (inplace) reverse vector\n",
    "\n",
    "$$\n",
    "A_i := A_{N - i + 1}, \\textrm{with } i = 1, \\ldots, N/2\n",
    "$$\n",
    "\n",
    "Big ideas:\n",
    "\n",
    "- thread independence\n",
    "- [race conditions](https://en.wikipedia.org/wiki/Race_condition)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ba789",
   "metadata": {},
   "source": [
    "## 2. CUDA Julia code on the tuckoo cluster\n",
    "\n",
    "As we have seen last [lecture](https://sdsu-comp605.github.io/spring25/lectures/module7-3_practical_cuda.html#tuckoo-demo-for-cuda-codes), we can run CUDA code on the tuckoo cluster. \n",
    "\n",
    "Now let's learn how to configure the `CUDA.jl` package to run with the proper CUDA version available on the cluster.\n",
    "\n",
    "Recall that for tuckoo, **`node8`** is the node dedicated for interactive usage, where you can find the `nvcc` compiler to compile CUDA codes.\n",
    "\n",
    "Once you are logged in on tuckoo using \n",
    "\n",
    "```shell\n",
    "ssh your_user_name@tuckoo.sdsu.edu\n",
    "```\n",
    "\n",
    "You will see that you are logged in on the _login node_ because the prompt will show\n",
    "\n",
    "```shell\n",
    "[your_user_name@tuckoo ~]$\n",
    "```\n",
    "\n",
    "Now you can use `rsh` to connect to `node8` by simply running:\n",
    "\n",
    "```shell\n",
    "rsh node8\n",
    "```\n",
    "\n",
    "and you will see that you are indeed on `node8` because the prompt has changed to:\n",
    "\n",
    "```shell\n",
    "[your_user_name@node8 ~]$\n",
    "```\n",
    "\n",
    "- Now you can start a Julia session with a local environment in a  directory of your choice, by running \n",
    "\n",
    "```julia\n",
    "julia --project=.\n",
    "```\n",
    "\n",
    "Recall that this will generate a `Project.toml` and `Manifest.toml` file in the directory in which you have specified the `--project=` path. \n",
    "\n",
    "- Once your Julia session has started, you can add the `CUDA.jl` and `Test.jl` packages needed to test your code. \n",
    "\n",
    "```julia\n",
    "Pkg.add(\"CUDA\")\n",
    "Pkg.add(\"Test\")\n",
    "```\n",
    "\n",
    "Now if you try to use the `CUDA.jl` package by running `using CUDA`, the first time you'll hit the following error:\n",
    "\n",
    "```julia\n",
    "julia> using CUDA\n",
    "┌ Error: CUDA.jl was precompiled without knowing the CUDA toolkit version. This is unsupported.\n",
    "│ You should either precompile CUDA.jl in an environment where the CUDA toolkit is available,\n",
    "│ or call `CUDA.set_runtime_version!` to specify which CUDA version to use.\n",
    "└ @ CUDA ~/.julia/packages/CUDA/TW8fL/src/initialization.jl:148\n",
    "```\n",
    "\n",
    "- Run the `CUDA.versioninfo()` command to see which CUDA version is available on the cluster:\n",
    "```julia\n",
    "julia> CUDA.versioninfo()\n",
    "```\n",
    "\n",
    "You will see the following output:\n",
    "\n",
    "```julia\n",
    "julia> CUDA.versioninfo()\n",
    "CUDA runtime 12.0, local installation\n",
    "CUDA driver 12.0\n",
    "NVIDIA driver 525.60.13\n",
    "\n",
    "CUDA libraries: \n",
    "- CUBLAS: 12.0.1\n",
    "- CURAND: 10.3.1\n",
    "- CUFFT: 11.0.0\n",
    "- CUSOLVER: 11.4.2\n",
    "- CUSPARSE: 12.0.0\n",
    "- CUPTI: 2022.4.0 (API 18.0.0)\n",
    "- NVML: 12.0.0+525.60.13\n",
    "\n",
    "Julia packages: \n",
    "- CUDA: 5.7.2\n",
    "- CUDA_Driver_jll: 0.12.1+1\n",
    "- CUDA_Runtime_jll: 0.16.1+0\n",
    "- CUDA_Runtime_Discovery: 0.3.5\n",
    "\n",
    "Toolchain:\n",
    "- Julia: 1.11.4\n",
    "- LLVM: 16.0.6\n",
    "\n",
    "Preferences:\n",
    "- CUDA_Runtime_jll.local: true\n",
    "\n",
    "2 devices:\n",
    "  0: Tesla P100-PCIE-16GB (sm_60, 15.892 GiB / 16.000 GiB available)\n",
    "  1: Tesla P100-PCIE-16GB (sm_60, 15.892 GiB / 16.000 GiB available)\n",
    "```\n",
    "\n",
    "- Therefore, we want to set the `v12.0` version for our `CUDA.jl` library. Let's do it by running the following command:\n",
    "\n",
    "```julia\n",
    "julia> CUDA.set_runtime_version!(v\"12.0\")\n",
    "```\n",
    "\n",
    "It will print the following output:\n",
    "\n",
    "```julia\n",
    "julia> CUDA.set_runtime_version!(v\"12.0\")\n",
    "[ Info: Configure the active project to use CUDA 12.0; please re-start Julia for this to take effect.\n",
    "```\n",
    "\n",
    "- Now your Julia environment on the cluster is properly setup to use the right CUDA version. \n",
    "\n",
    "- You will notice that your local `Project.toml` and `Manifest.toml` files have changed and now include the `CUDA.jl` dependency. \n",
    "\n",
    "### 2.1 Launching a CUDA.jl code with SLURM\n",
    "\n",
    "Once you have verified that your local `Project.toml` and `Manifest.toml` files have been populated correctly, you can copy the [julia_codes/module8-1/copy_cu_array.jl](https://github.com/sdsu-comp605/spring25/blob/main/julia_codes/module8-1/copy_cu_array.jl) and [batch_scripts/batch.cuda-julia](https://github.com/sdsu-comp605/spring25/tree/main/batch_scripts/batch.cuda-julia) files in the same directory where you have the `Project.toml` and `Manifest.toml` files.\n",
    "\n",
    "This is the SLURM batch script to setup and precompile your Julia environment and launch Julia CODE on a P100 on tuckoo:\n",
    "\n",
    "```{literalinclude} ../batch_scripts/batch.cuda-julia\n",
    ":language: shell\n",
    ":linenos: true\n",
    "```\n",
    "\n",
    "\n",
    "You can simply launch your SLURM batch script with\n",
    "\n",
    "```shell\n",
    "sbatch batch.cuda-julia\n",
    "```\n",
    "\n",
    "and it should produce the following output:\n",
    "\n",
    "```shell\n",
    "Status `~/comp605/spring25/julia_codes/module8-1/Project.toml`\n",
    "  [052768ef] CUDA v5.7.3\n",
    "  [8dfed614] Test v1.11.0\n",
    "Begin test. \n",
    "Test ended. \n",
    "Test Summary: | Pass  Total  Time\n",
    "CopyCuArray   |    1      1  1.0s\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
