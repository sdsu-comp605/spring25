{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca20e011",
   "metadata": {},
   "source": [
    "# 29) Memory management with CUDA.jl\n",
    "\n",
    "Last time:\n",
    "- Introduction to CUDA.jl\n",
    "\n",
    "Today:\n",
    "\n",
    "1. Outline\n",
    "2. Problem statement\n",
    "3. Simple kernels using global memory  \n",
    "4. Simple kernels using shared/local memory  \n",
    "5. Instruction Level Parallelism (both with and without local memory)  \n",
    "6. Bank Conflicts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b53366c",
   "metadata": {},
   "source": [
    "## 1. Outline\n",
    "\n",
    "In this lecture, we will use efficient matrix transpose on the GPU as an example to illustrate:\n",
    "\n",
    "- global memory access (`CuArrays`)\n",
    "- shared memory usage (`@cuStaticSharedMem`)\n",
    "- memory bank conflicts\n",
    "- instruction level parallelism\n",
    "\n",
    ":::{note} References:\n",
    "- We will follow the outlines of [An Efficient Matrix Transpose in CUDA C/C++](https://devblogs.nvidia.com/parallelforall/efficient-matrix-transpose-cuda-cc/)\n",
    "by Mark Harris. Though this is an example in CUDA C, the strategy follows\n",
    "through directly in Julia as well.\n",
    "\n",
    "- We will also reference [How to Access Global Memory Efficiently in CUDA C/C++ Kernels](https://devblogs.nvidia.com/parallelforall/how-access-global-memory-efficiently-cuda-c-kernels/).\n",
    ":::\n",
    "\n",
    "The code for the matrix transpose can be found in [julia_codes/module8-2/transpose.jl](https://github.com/sdsu-comp605/spring25/blob/main/julia_codes/module8-2/transpose.jl)\n",
    "\n",
    "## 2. Problem statement:\n",
    "\n",
    "To do an efficient matrix transpose, \n",
    "$$\n",
    "Y = X^T, \n",
    "$$\n",
    "on the GPU, we will be looking at two operations:\n",
    "\n",
    "- **copy**: `y[i, j] = x[i, j]`\n",
    "    - Coalesced memory access on the read and write\n",
    "- **transpose**: `y[j, i] = x[i, j]`\n",
    "    - Coalesced memory access on the read but not the write\n",
    "    - Coalesced memory access on the write but not the read\n",
    "\n",
    "But before doing that, we need to talk about array indexing.\n",
    "\n",
    "### A typical GPU-porting workflow\n",
    "\n",
    "A typical approach for porting or developing an application for the GPU is as follows:\n",
    "\n",
    "1. develop an application using generic array functionality, and test it on the CPU with the Array type\n",
    "2. port your application to the GPU by switching to the CuArray type\n",
    "3. disallow the CPU fallback (\"scalar indexing\") to find operations that are not implemented for or incompatible with GPU execution\n",
    "4. (optional) use lower-level, CUDA-specific interfaces to implement missing functionality or optimize performance\n",
    "\n",
    "### Scalar indexing\n",
    "\n",
    "Many array operations in Julia are implemented using loops, processing one element at a time. \n",
    "\n",
    "Doing so with GPU arrays is _very ineffective_, as the loop won't actually execute on the GPU, but transfer one element at a time and process it on the CPU. As this wrecks performance, you will be warned when performing this kind of iteration with the following error message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b280715",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA\n",
    "\n",
    "a = CuArray([1])\n",
    "\n",
    "a[1] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629e17c3",
   "metadata": {},
   "source": [
    "Scalar indexing is only allowed in an interactive session, e.g. the REPL, because it is convenient when porting CPU code to the GPU and for debugging purposes. \n",
    "\n",
    "If you want to disallow scalar indexing, e.g. to verify that your application executes correctly on the GPU, call the `allowscalar` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9b11ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.allowscalar(false)\n",
    "\n",
    "a[1] .+ 1 # this will error again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8624a7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but this will work:\n",
    "\n",
    "a .+ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955de379",
   "metadata": {},
   "source": [
    "In a non-interactive session, e.g. when running code from a script or application, scalar indexing is _disallowed by default_. \n",
    "\n",
    "There is no global toggle to allow scalar indexing; if you really need it, you can mark expressions using `allowscalar` with do-block syntax or `@allowscalar` macro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d9e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = CuArray([1])\n",
    "\n",
    "CUDA.allowscalar() do\n",
    "    a[1] += 1\n",
    "  end\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b6fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.@allowscalar a[1] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fedcc5",
   "metadata": {},
   "source": [
    "## 3. Simple kernels using global memory \n",
    "\n",
    "Recall that **global memory** resides in device memory and device memory is accessed via 32-, 64-, or 128-byte memory transactions. These memory transactions must be naturally aligned: Only the 32-, 64-, or 128-byte segments of device memory that are aligned to their size (i.e., whose first address is a multiple of their size) can be read or written by memory transactions.\n",
    "\n",
    "Examples of global memory accesses:\n",
    "\n",
    "```julia\n",
    "# Simple routines using global memory\n",
    "function copy_naive!(b, a)\n",
    "  N = size(a, 1)\n",
    "  i = (blockIdx().x-1) * TILE_DIM + threadIdx().x\n",
    "  j = (blockIdx().y-1) * TILE_DIM + threadIdx().y\n",
    "\n",
    "  @inbounds if i <= N && j <= N\n",
    "    b[i, j] = a[i, j]\n",
    "  end\n",
    "  nothing\n",
    "end\n",
    "\n",
    "function transpose_naive!(b, a)\n",
    "  N = size(a, 1)\n",
    "  i = (blockIdx().x-1) * TILE_DIM + threadIdx().x\n",
    "  j = (blockIdx().y-1) * TILE_DIM + threadIdx().y\n",
    "\n",
    "  @inbounds if i <= N && j <= N\n",
    "    b[j, i] = a[i, j]\n",
    "  end\n",
    "  nothing\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d51522",
   "metadata": {},
   "source": [
    "## 3. Simple kernels using shared/local memory\n",
    "\n",
    "CUDA allows us to allocate shared memory which all threads in a thread block can access. \n",
    "\n",
    "Shared memory is a fast, on-chip memory that is accessible by all threads within a single CUDA thread block. It's designed to facilitate efficient data sharing between threads within a block. \n",
    "\n",
    "By using shared memory we can have _coalesced_ reads and writes from global memory (reads from shared memory may not be coalesced: see [Bank Conflicts](#bank-conflicts) below). \n",
    "\n",
    "The syntax for shared memory is `cuStaticSharedMem` and for convenience is as a double array:\n",
    "\n",
    "```julia\n",
    "tile = @cuStaticSharedMem(Float64, (SIZE_X, SIZE_Y))\n",
    "```\n",
    "\n",
    "where the first index is the continuous/fastest index, e.g.,\n",
    "\n",
    "```julia\n",
    "tile[i, j] -> loc_x[i + (j-1) * SIZE_X]\n",
    "```\n",
    "\n",
    "- Access to local memory on the GPU is faster than global memory (though not as fast as registers).\n",
    "\n",
    "**Note**: The size of the array `SIZE_X` and `SIZE_Y` must be known at **compile time**, thus it must be either a global constant or passed into the kernel through a `Val` (recall that we have seen this in [lecture 8](https://sdsu-comp605.github.io/spring25/lectures/module2-3_blocked_mmm.html#compile-time-constants-val)) or see the documentation `?Val` for how to pass constants through to functions at compile time.\n",
    "\n",
    "**Strategy**: Load data first into the local shared memory, then write out from local shared memory to global memory (so that both the reads and writes are coalesced into global memory)\n",
    "\n",
    "```julia\n",
    "#=\n",
    "Simple routines using shared memory\n",
    "Idea is that we first load a patch of x into shared memory then write out from\n",
    "shared memory\n",
    "=#\n",
    "function copy_shared!(b, a)\n",
    "  N = size(a, 1)\n",
    "  tidx, tidy = threadIdx().x, threadIdx().y\n",
    "  i = (blockIdx().x-1) * TILE_DIM + tidx\n",
    "  j = (blockIdx().y-1) * TILE_DIM + tidy\n",
    "  tile = @cuStaticSharedMem(eltype(a), (TILE_DIM, TILE_DIM))\n",
    "\n",
    "  @inbounds if i <= N && j <= N\n",
    "    tile[tidx, tidy] = a[i, j]\n",
    "  end\n",
    "\n",
    "  sync_threads()\n",
    "\n",
    "  @inbounds if i <= N && j <= N\n",
    "    b[i, j] = tile[tidx, tidy]\n",
    "  end\n",
    "\n",
    "  nothing\n",
    "end\n",
    "\n",
    "function transpose_shared!(b, a)\n",
    "  N = size(a, 1)\n",
    "  tidx, tidy = threadIdx().x, threadIdx().y\n",
    "  bidx, bidy = blockIdx().x, blockIdx().y\n",
    "  i = (bidx-1) * TILE_DIM + tidx\n",
    "  j = (bidy-1) * TILE_DIM + tidy\n",
    "  tile = @cuStaticSharedMem(eltype(a), (TILE_DIM, TILE_DIM))\n",
    "\n",
    "  @inbounds if i <= N && j <= N\n",
    "    tile[tidx, tidy] = a[i, j]\n",
    "  end\n",
    "\n",
    "  sync_threads()\n",
    "\n",
    "  i = (bidy-1) * TILE_DIM + tidx\n",
    "  j = (bidx-1) * TILE_DIM + tidy\n",
    "\n",
    "  @inbounds if i <= N && j <= N\n",
    "    b[i, j] = tile[tidy, tidx]\n",
    "  end\n",
    "\n",
    "  nothing\n",
    "end\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef05ec01",
   "metadata": {},
   "source": [
    "## 4. Instruction Level Parallelism (both with and without local memory)\n",
    "\n",
    "Instruction level parallelism is the ability of the hardware to execute (in parallel) multiple, independent instructions from the same thread; floating point operations take `~4 cycles` before completing.\n",
    "\n",
    "- Recall that NVIDIA architectures follow the \"[**Single instruction, multiple threads (SIMT)**](https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads#) paradigm to achieve instruction level parallelism.\n",
    "\n",
    "Ideas:\n",
    "\n",
    "```julia\n",
    "# Non-parallel instructions\n",
    "x = a * b * c;\n",
    "# stall\n",
    "y = x * x;\n",
    "\n",
    "# Instruction level parallelizable\n",
    "x = a * b * c;\n",
    "y = a + b + c; // independent of x\n",
    "# not stall\n",
    "z = x * y\n",
    "```\n",
    "\n",
    "\n",
    "This applies to memory access too! If we access memory the thread only stalls once we need the memory\n",
    "\n",
    "```julia\n",
    "loc = glo_x[n];\n",
    "\n",
    "#=\n",
    "lots of work not involving loc...\n",
    "=#\n",
    "\n",
    "# stall until memory access is done\n",
    "loc = loc * loc;\n",
    "```\n",
    "\n",
    "\n",
    "> To see how this plays out in practice on the GPU see this [forum thread](https://devtalk.nvidia.com/default/topic/841359/cuda-programming-and-performance/instruction-level-parallelism/post/4562896/#4562896) as an example.\n",
    "\n",
    "How to use all of this for the transpose? Have one thread issue several calls read and write from global memory and shared memory. (In this case *I think* the performance boost is coming from:\n",
    "\n",
    "- (1) fewer registers being used, and thus more thread being scheduled and \n",
    "- (2) the scheduling on this card is such that there are a few extra loads being issued at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8413b9b",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "```julia\n",
    "#=\n",
    "Tiled routines using global memory:\n",
    "Idea here is that we break up the matrix into blocks of size\n",
    "\n",
    "   [TILE_DIM, TILE_DIM]\n",
    "\n",
    "We then copy the data over in chunks of size\n",
    "\n",
    "   [TILE_DIM, BLOCK_ROWS]\n",
    "\n",
    "so we have to copy TILE_DIM / BLOCK_ROWS chunks to loop over. For example, if TILE_DIM = 8 and BLOCK_ROWS = 2 then the data is copied in this order in the transposed matrix (where the number indicates the iteration of the for loop the data is filled by)\n",
    "\n",
    "  0 0 1 1 2 2 3 3\n",
    "  0 0 1 1 2 2 3 3\n",
    "  0 0 1 1 2 2 3 3\n",
    "  0 0 1 1 2 2 3 3\n",
    "  0 0 1 1 2 2 3 3\n",
    "  0 0 1 1 2 2 3 3\n",
    "  0 0 1 1 2 2 3 3\n",
    "  0 0 1 1 2 2 3 3\n",
    "=#\n",
    "\n",
    "function copy_tiled!(b, a)\n",
    "  N = size(a, 1)\n",
    "  i = (blockIdx().x-1) * TILE_DIM + threadIdx().x\n",
    "  j = (blockIdx().y-1) * TILE_DIM + threadIdx().y\n",
    "\n",
    "  @inbounds for k = 0:BLOCK_ROWS:TILE_DIM-1\n",
    "    if i <= N && (j+k) <= N\n",
    "      b[i, j+k] = a[i, j+k]\n",
    "    end\n",
    "  end\n",
    "end\n",
    "\n",
    "function transpose_tiled!(b, a)\n",
    "  N = size(a, 1)\n",
    "  i = (blockIdx().x-1) * TILE_DIM + threadIdx().x\n",
    "  j = (blockIdx().y-1) * TILE_DIM + threadIdx().y\n",
    "\n",
    "  @inbounds for k = 0:BLOCK_ROWS:TILE_DIM-1\n",
    "    if i <= N && (j+k) <= N\n",
    "      b[j+k, i] = a[i, j+k]\n",
    "    end\n",
    "  end\n",
    "end\n",
    "\n",
    "#=\n",
    "Tiled routines using shared memory:\n",
    "Combine the tiling idea with shared memory\n",
    "=#\n",
    "function copy_tiled_shared!(b, a)\n",
    "  N = size(a, 1)\n",
    "  tidx, tidy = threadIdx().x, threadIdx().y\n",
    "  i = (blockIdx().x-1) * TILE_DIM + tidx\n",
    "  j = (blockIdx().y-1) * TILE_DIM + tidy\n",
    "\n",
    "  tile = @cuStaticSharedMem(eltype(a), (TILE_DIM, TILE_DIM))\n",
    "\n",
    "  @inbounds for k = 0:BLOCK_ROWS:TILE_DIM-1\n",
    "    if i <= N && (j+k) <= N\n",
    "      tile[tidx, tidy+k] = a[i, j+k]\n",
    "    end\n",
    "  end\n",
    "\n",
    "  sync_threads()\n",
    "\n",
    "  @inbounds for k = 0:BLOCK_ROWS:TILE_DIM-1\n",
    "    if i <= N && (j+k) <= N\n",
    "      b[i, j+k] = tile[tidx, tidy+k]\n",
    "    end\n",
    "  end\n",
    "\n",
    "  nothing\n",
    "end\n",
    "\n",
    "function transpose_tiled_shared!(b, a)\n",
    "  N = size(a, 1)\n",
    "  tidx, tidy = threadIdx().x, threadIdx().y\n",
    "  i = (blockIdx().x-1) * TILE_DIM + tidx\n",
    "  j = (blockIdx().y-1) * TILE_DIM + tidy\n",
    "\n",
    "  tile = @cuStaticSharedMem(eltype(a), (TILE_DIM, TILE_DIM))\n",
    "\n",
    "  @inbounds for k = 0:BLOCK_ROWS:TILE_DIM-1\n",
    "    if i <= N && (j+k) <= N\n",
    "      tile[tidx, tidy+k] = a[i, j+k]\n",
    "    end\n",
    "  end\n",
    "\n",
    "  sync_threads()\n",
    "\n",
    "  i = (blockIdx().y-1) * TILE_DIM + tidx\n",
    "  j = (blockIdx().x-1) * TILE_DIM + tidy\n",
    "  @inbounds for k = 0:BLOCK_ROWS:TILE_DIM-1\n",
    "    if i <= N && (j+k) <= N\n",
    "      b[i, j+k] = tile[tidy+k, tidx]\n",
    "    end\n",
    "  end\n",
    "\n",
    "  nothing\n",
    "end\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeeab75",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 5. Bank Conflicts\n",
    "\n",
    "Local memory on most (perhaps all?) GPUs is organized into memory banks (16 in the example below, but it could be 32 most likely on modern GPUs) where each bank is 4 bytes (on NVIDIA Kepler cards this can be changed to 8 bytes inside of CUDA; but default is 4 bytes).\n",
    "\n",
    "```julia\n",
    "Bank |      0     |      1      |      2      | ... |     16\n",
    "Byte | 0  1  2  3 |  4  5  6  7 |  8  9 10 11 | ... | 28 29 30 31\n",
    "     |32 33 34 35 | 36 37 38 39 | 40 41 42 43 | ... | 60 61 62 63\n",
    "     |64 65 66 67 | 68 69 70 71 | 72 73 74 75 | ... | 92 93 94 95\n",
    "```\n",
    "\n",
    "Each bank has a bandwidth of 32 bits per clock cycle. This means each bank can hold 4 bytes of data (32 bits). \n",
    "\n",
    "The limitation is that if two threads in a warp (group of 32 threads on NVIDIA cards) access different locations of the same bank, the access will be serialized (i.e., take more than one memory access call). In fact, when multiple threads within a warp (a group of 32 threads) attempt to access the same bank simultaneously, a **bank conflict** occurs. This can lead to a performance penalty as the accesses need to be serialized, slowing down the memory access. \n",
    "\n",
    "Since it's optimal to have our work groups as multiples of `32`, it is also optimal to make sure our shared memory access is _not_ a multiple of `32`, thus we want to make the fastest dimension a little faster for the shared memory so two threads don't access the same bank.\n",
    "\n",
    "Example:\n",
    "\n",
    "```julia\n",
    "#=\n",
    "Tiled routines using shared memory:\n",
    "Combine the tiling idea with shared memory, with no bank conflicts\n",
    "=#\n",
    "function transpose_tiled_shared_noconflicts!(b, a)\n",
    "  N = size(a, 1)\n",
    "  tidx, tidy = threadIdx().x, threadIdx().y\n",
    "  i = (blockIdx().x-1) * TILE_DIM + tidx\n",
    "  j = (blockIdx().y-1) * TILE_DIM + tidy\n",
    "\n",
    "  tile = @cuStaticSharedMem(eltype(a), (TILE_DIM+1, TILE_DIM)) # note the different TILE_DIM+1\n",
    "\n",
    "  @inbounds for k = 0:BLOCK_ROWS:TILE_DIM-1\n",
    "    if i <= N && (j+k) <= N\n",
    "      tile[tidx, tidy+k] = a[i, j+k]\n",
    "    end\n",
    "  end\n",
    "\n",
    "  sync_threads()\n",
    "\n",
    "  i = (blockIdx().y-1) * TILE_DIM + tidx\n",
    "  j = (blockIdx().x-1) * TILE_DIM + tidy\n",
    "  @inbounds for k = 0:BLOCK_ROWS:TILE_DIM-1\n",
    "    if i <= N && (j+k) <= N\n",
    "      b[i, j+k] = tile[tidy+k, tidx]\n",
    "    end\n",
    "  end\n",
    "\n",
    "  nothing\n",
    "end\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.6",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
