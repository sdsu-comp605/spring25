{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615027b2",
   "metadata": {},
   "source": [
    "# 24) HW3 solution\n",
    "\n",
    "## Part 1: Optimizing dot product (30%)\n",
    "\n",
    "(5%) The compile/runtime unrolling factor versions achieve performance similar to the `dot_opt2` version, but not as good as the `dot_opt3` version. \n",
    "\n",
    "In addition to your commentary, answer the following questions:\n",
    "\n",
    "1. (5%) Is this code correct for all values of n?\n",
    "    - No, for odd `n`, we incur in out-of-bound access when we perform the second product, `a[i+1] * b[i+1]`.\n",
    "2. (5%) Can this code change the numerically computed result?\n",
    "    - Yes, because of floating-point aritmethic (FPA). We're adding an extra operation. If any of the numbers to add were exrtemely small (or large), we will suffer from roundoff errors. Also, as you've learned, the C standard does not allow floating point arithmetic to be reordered because it may change the computed values (non associativity).\n",
    "3. (5%) Can you extend this technique to increase performance further (longer vector registers and/or more instruction-level parallelism)?\n",
    "    - Yes, we can definitely consider instead of an unrolling factor 2, chunking the data in larger sizes and apply SIMD-like operations (like we do in Part 2)\n",
    "4. (5%) Can you make the unrolling factor 2 a compile-time constant, perhaps by using an inner loop?\n",
    "    - Yes, we can do that (see `dot_opt_comp_time_m` function)\n",
    "5. (5%) Could that unrolling factor be a run-time parameter?\n",
    "    - Yes, we can do that (see `dot_opt_run_time_m` function).\n",
    "    There are a couple of different ways this can be achieved. \n",
    "      * If someone defined the runtime parameter, in say, `main`, then the compiler would get \"smart\" and optimize and treat it in the same way as a compile-time constant. Keep in mind that we're compiling with the optimization flag `-O3`, so if we only used that unrolling factor in the call to our function in `main`, then the the compiler knows at compile time the value that we're passing, and because we're calling that function only once, it might just substitute the runtime constant as it were a constant known at compile time\n",
    "      \n",
    "      * But if someone actually added it as a true runtime unrolling factor, passed via a command-line argument, which the compiler cannot know in advance, then, yes, the performance of the runtime version would be lower than the compile-time counterpart.  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807c4b65",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! gcc -O3 -march=native -fopenmp    dot.c   -o dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea0d33c-05d2-4b57-a118-15056cb5cc15",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! OMP_NUM_THREADS=4 ./dot -r 10 -n 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10441247",
   "metadata": {},
   "source": [
    "## Part 2: Optimizing block inner product (%70)\n",
    "\n",
    "- What does it mean to reorder loops? Will it help or hurt performance?\n",
    "\n",
    "\n",
    "The two lines that originally use the `j` as slowest index and `k` as fastest index to perform the pair-wise products:\n",
    "\n",
    "```\n",
    "for (size_t j=0; j<J; j++) {\n",
    "    for (size_t k=0; k<K; k++) {\n",
    "```\n",
    "\n",
    "tell us that the same row of the matrix $A^T$ is multiplied by all columns of $B$, and we store the result in $C$ row-wise. \n",
    "\n",
    "If we swapped these two lines, (see the `bdot_opt3_simd_reordered` function where we do this), we would multiply the same column of $B$ by all rows of $A^T$ and we'd store the result in $C$ column-wise. \n",
    "\n",
    "The effect of doing so it is actually negligible on performance (on my architecture) and might bring only a very minimal advantage on some other architectures. If you think about it, this would only change how 32 pair-wise dot products are performed, but the majority of the work in this code is done in the single pair-wise dot products (20000 FLOPs each).\n",
    "\n",
    "- Does it help to change the layout in memory (see the `aistride`, `ajstride`, `bistride`, and `bkstride` parameters in the code)?\n",
    "\n",
    "No, doing it without further algorithmic changes in the functions where we actually perform the dot products, does not help. First of all, the only way the stride parameters can be changed is in the call of the `init_bdot` function. Instead of calling `init_bdot` as we called it originally:\n",
    "\n",
    "```\n",
    "init_bdot(args.length, a, n, 1, b, 1, n);\n",
    "```\n",
    "\n",
    "one of the two arguments between `n` and `1` has to be kept to `1` to fill the whole matrices `A` and `B` without any gaps, as in:\n",
    "\n",
    "```\n",
    "init_bdot(args.length, a, 1, n, b, n, 1);\n",
    "```\n",
    "\n",
    "This would make `A` be stored column-wise and `B` stored row-wise (effectively transposing the matrices). If we did this, in order for the algorithm to still produce the same results of the pairwise dot products, we would also need to perform further algorithmic changes. Even if we did perform those further algorithmic changes to make the results correct, it would not help performance, as we would be back to the original situation.\n",
    "\n",
    "One might have also thought to use smaller blocks/stride patterns, such as `1, J` and `1, K`. However, again, this would only change how the data entries are laid out, and without further algorithmic changes in the functions that actually call the pair-wise dot products (which are implemented to perform the dot product of the _entire_ vectors), it would not be beneficial.\n",
    "\n",
    "- Try using the `#pragma omp simd` directive seen in class and the compiler option `-fopenmp-simd`.\n",
    "\n",
    "See all the `bdot_opt*_simd` versions and related results (where `*` is 1,2,3). The addition of the `#pragma omp simd` directive only significantly helps the `bdot_opt3` optimized version, and leaves the `bdot_opt1` and `bdot_opt2` results essentially unaltered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8896a17e-aced-4697-924a-443042d84b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcc -O3 -march=native -fopenmp-simd -fopenmp dot.c -o dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9943536-a314-4fe9-873c-cb352ae068c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! OMP_NUM_THREADS=4 ./dot -r 10 -n 10000 -b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1933188",
   "metadata": {},
   "source": [
    "## Code:\n",
    "\n",
    "```{literalinclude} ../c_codes/hw3_solution/dot.c\n",
    ":language: c\n",
    ":linenos: true\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
